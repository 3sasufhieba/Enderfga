{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入包以及设置随机种子\n",
    "%pip install -r requirements.txt\n",
    "import numpy as np\n",
    "import torchvision as tv\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "seed = 42\n",
    "t.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "t.cuda.manual_seed_all(seed)\n",
    "t.backends.cudnn.deterministic = True\n",
    "t.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义超参数类\n",
    "class argparse():\n",
    "    pass\n",
    "\n",
    "args = argparse()\n",
    "args.epochs, args.learning_rate, args.patience = [30, 0.001, 4]\n",
    "args.batch_size = 16\n",
    "args.save_path = 'model/'\n",
    "args.device, = [t.device(\"cuda:{}\".format(0) if t.cuda.is_available() else \"cpu\"),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None, **kwargs):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,\n",
    "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    注意：原论文中，在虚线残差结构的主分支上，第一个1x1卷积层的步距是2，第二个3x3卷积层步距是1。\n",
    "    但在pytorch官方实现过程中是第一个1x1卷积层的步距是1，第二个3x3卷积层步距是2，\n",
    "    这么做的好处是能够在top1上提升大概0.5%的准确率。\n",
    "    可参考Resnet v1.5 https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channel, out_channel, stride=1, downsample=None,\n",
    "                 groups=1, width_per_group=64):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        width = int(out_channel * (width_per_group / 64.)) * groups\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,\n",
    "                               kernel_size=1, stride=1, bias=False)  # squeeze channels\n",
    "        self.bn1 = nn.BatchNorm2d(width)\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,\n",
    "                               kernel_size=3, stride=stride, bias=False, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(width)\n",
    "        # -----------------------------------------\n",
    "        self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel*self.expansion,\n",
    "                               kernel_size=1, stride=1, bias=False)  # unsqueeze channels\n",
    "        self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 blocks_num,\n",
    "                 num_classes=1000,\n",
    "                 include_top=True,\n",
    "                 groups=1,\n",
    "                 width_per_group=64):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.include_top = include_top\n",
    "        self.in_channel = 64\n",
    "\n",
    "        self.groups = groups\n",
    "        self.width_per_group = width_per_group\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, self.in_channel, kernel_size=7, stride=2,\n",
    "                               padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channel)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, blocks_num[0])\n",
    "        self.layer2 = self._make_layer(block, 128, blocks_num[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, blocks_num[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, blocks_num[3], stride=2)\n",
    "        if self.include_top:\n",
    "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # output size = (1, 1)\n",
    "            self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def _make_layer(self, block, channel, block_num, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channel != channel * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(channel * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channel,\n",
    "                            channel,\n",
    "                            downsample=downsample,\n",
    "                            stride=stride,\n",
    "                            groups=self.groups,\n",
    "                            width_per_group=self.width_per_group))\n",
    "        self.in_channel = channel * block.expansion\n",
    "\n",
    "        for _ in range(1, block_num):\n",
    "            layers.append(block(self.in_channel,\n",
    "                                channel,\n",
    "                                groups=self.groups,\n",
    "                                width_per_group=self.width_per_group))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        if self.include_top:\n",
    "            x = self.avgpool(x)\n",
    "            x = t.flatten(x, 1)\n",
    "            x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def resnet34(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet34-333f7ec4.pth\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
    "\n",
    "\n",
    "def resnet50(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, include_top=include_top)\n",
    "\n",
    "\n",
    "def resnet101(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, include_top=include_top)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\n",
    "    groups = 32\n",
    "    width_per_group = 4\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3],\n",
    "                  num_classes=num_classes,\n",
    "                  include_top=include_top,\n",
    "                  groups=groups,\n",
    "                  width_per_group=width_per_group)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(num_classes=1000, include_top=True):\n",
    "    # https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\n",
    "    groups = 32\n",
    "    width_per_group = 8\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3],\n",
    "                  num_classes=num_classes,\n",
    "                  include_top=include_top,\n",
    "                  groups=groups,\n",
    "                  width_per_group=width_per_group)\n",
    "net = resnet34(10).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义早停类\n",
    "class EarlyStopping():\n",
    "    def __init__(self,patience=7,verbose=False,delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "    def __call__(self,val_loss,model,path):\n",
    "        print(\"val_loss={}\".format(val_loss))\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss,model,path)\n",
    "        elif score < self.best_score+self.delta:\n",
    "            self.counter+=1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter>=self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss,model,path)\n",
    "            self.counter = 0\n",
    "    def save_checkpoint(self,val_loss,model,path):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        t.save(model.state_dict(), path+'/'+'model_checkpoint.pth')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载数据集Dataset,DataLoader\n",
    "# 定义对数据的预处理\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # 转为Tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化\n",
    "                             ])\n",
    "\n",
    "\n",
    "\n",
    "trainset = tv.datasets.ImageFolder(root='./data/cifar/train',transform=transform)\n",
    "\n",
    "trainloader = t.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = tv.datasets.ImageFolder(root='./data/cifar/test',  transform=transform)\n",
    "\n",
    "testloader = t.utils.data.DataLoader(testset, batch_size=args.batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置loss，优化器等\n",
    "criterion = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "optimizer = optim.Adam(net.parameters(), lr=args.learning_rate) # 优化器\n",
    "early_stopping = EarlyStopping(patience=args.patience,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练模型\n",
    "def train_model(model, batch_size, patience, n_epochs):\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # 训练模式\n",
    "        for batch, (data, target) in enumerate(trainloader, 1):\n",
    "            # 把数据存到GPU上\n",
    "            data = data.to(args.device)\n",
    "            target = target.to(args.device)\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            # 计算loss\n",
    "            loss = criterion(output, target)\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            # 记录训练loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # 切换到验证模式\n",
    "        for data, target in testloader:\n",
    "            # 把数据存到GPU上\n",
    "            data = data.to(args.device)\n",
    "            target = target.to(args.device)\n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            # 计算loss\n",
    "            loss = criterion(output, target)\n",
    "            # 记录验证loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # 计算平均loss\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # 下个epoch前清空loss\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        #==================early stopping======================\n",
    "        early_stopping(avg_valid_losses[-1],model=model,path='model')\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        #====================adjust lr========================\n",
    "        lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    return  model, avg_train_losses, avg_valid_losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/30] train_loss: 2.00228 valid_loss: 1.87363\n",
      "val_loss=1.8736258753776551\n",
      "Validation loss decreased (inf --> 1.873626).  Saving model ...\n",
      "[ 2/30] train_loss: 1.71378 valid_loss: 1.62153\n",
      "val_loss=1.6215307261943817\n",
      "Validation loss decreased (1.873626 --> 1.621531).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "[ 3/30] train_loss: 1.37990 valid_loss: 1.31292\n",
      "val_loss=1.3129229586601256\n",
      "Validation loss decreased (1.621531 --> 1.312923).  Saving model ...\n",
      "[ 4/30] train_loss: 1.27232 valid_loss: 1.26039\n",
      "val_loss=1.2603876221179962\n",
      "Validation loss decreased (1.312923 --> 1.260388).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "[ 5/30] train_loss: 1.17021 valid_loss: 1.23929\n",
      "val_loss=1.2392868629932403\n",
      "Validation loss decreased (1.260388 --> 1.239287).  Saving model ...\n",
      "[ 6/30] train_loss: 1.16020 valid_loss: 1.23648\n",
      "val_loss=1.2364766626358032\n",
      "Validation loss decreased (1.239287 --> 1.236477).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "[ 7/30] train_loss: 1.12742 valid_loss: 1.22745\n",
      "val_loss=1.2274490322589875\n",
      "Validation loss decreased (1.236477 --> 1.227449).  Saving model ...\n",
      "[ 8/30] train_loss: 1.12007 valid_loss: 1.22169\n",
      "val_loss=1.2216905331611634\n",
      "Validation loss decreased (1.227449 --> 1.221691).  Saving model ...\n",
      "Updating learning rate to 1e-06\n",
      "[ 9/30] train_loss: 1.10339 valid_loss: 1.22175\n",
      "val_loss=1.2217504484653472\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[10/30] train_loss: 1.09392 valid_loss: 1.21521\n",
      "val_loss=1.215206212902069\n",
      "Validation loss decreased (1.221691 --> 1.215206).  Saving model ...\n",
      "Updating learning rate to 5e-07\n",
      "[11/30] train_loss: 1.08812 valid_loss: 1.21754\n",
      "val_loss=1.2175404346466066\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[12/30] train_loss: 1.10143 valid_loss: 1.21537\n",
      "val_loss=1.2153696400642395\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[13/30] train_loss: 1.08686 valid_loss: 1.21353\n",
      "val_loss=1.2135331664562226\n",
      "Validation loss decreased (1.215206 --> 1.213533).  Saving model ...\n",
      "[14/30] train_loss: 1.10102 valid_loss: 1.21850\n",
      "val_loss=1.2185046174526215\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[15/30] train_loss: 1.09588 valid_loss: 1.21474\n",
      "val_loss=1.2147417855262757\n",
      "EarlyStopping counter: 2 out of 4\n",
      "Updating learning rate to 1e-07\n",
      "[16/30] train_loss: 1.09867 valid_loss: 1.21383\n",
      "val_loss=1.2138345208644867\n",
      "EarlyStopping counter: 3 out of 4\n",
      "[17/30] train_loss: 1.08206 valid_loss: 1.21863\n",
      "val_loss=1.218633563041687\n",
      "EarlyStopping counter: 4 out of 4\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# baseline训练 使用了batchnorm，learningrate_decay,earlystopping\n",
    "model, train_losses, valid_losses = train_model(net, args.batch_size, args.patience, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#训练可视化\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(valid_losses, label='valid loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy():\n",
    "    correct = 0 # 预测正确的图片数\n",
    "    total = 0 # 总共的图片数\n",
    "\n",
    "\n",
    "    # 由于测试的时候不需要求导，可以暂时关闭autograd，提高速度，节约内存\n",
    "    with t.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(args.device)\n",
    "            labels = labels.to(args.device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = t.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "    print('10000张测试集中的准确率为: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000张测试集中的准确率为: 45%\n"
     ]
    }
   ],
   "source": [
    "# baseline测试\n",
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/30] train_loss: 2.08010 valid_loss: 2.22998\n",
      "val_loss=2.229979174232483\n",
      "Validation loss decreased (inf --> 2.229979).  Saving model ...\n",
      "[ 2/30] train_loss: 1.92616 valid_loss: 2.12214\n",
      "val_loss=2.1221358406066892\n",
      "Validation loss decreased (2.229979 --> 2.122136).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "[ 3/30] train_loss: 1.80495 valid_loss: 1.61325\n",
      "val_loss=1.613250291442871\n",
      "Validation loss decreased (2.122136 --> 1.613250).  Saving model ...\n",
      "[ 4/30] train_loss: 1.67074 valid_loss: 1.53172\n",
      "val_loss=1.531720094013214\n",
      "Validation loss decreased (1.613250 --> 1.531720).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "[ 5/30] train_loss: 1.61074 valid_loss: 1.48498\n",
      "val_loss=1.4849812232017516\n",
      "Validation loss decreased (1.531720 --> 1.484981).  Saving model ...\n",
      "[ 6/30] train_loss: 1.60771 valid_loss: 1.46946\n",
      "val_loss=1.4694588244438171\n",
      "Validation loss decreased (1.484981 --> 1.469459).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "[ 7/30] train_loss: 1.57967 valid_loss: 1.46505\n",
      "val_loss=1.4650538705825806\n",
      "Validation loss decreased (1.469459 --> 1.465054).  Saving model ...\n",
      "[ 8/30] train_loss: 1.57740 valid_loss: 1.44637\n",
      "val_loss=1.446365863609314\n",
      "Validation loss decreased (1.465054 --> 1.446366).  Saving model ...\n",
      "Updating learning rate to 1e-06\n",
      "[ 9/30] train_loss: 1.57319 valid_loss: 1.45719\n",
      "val_loss=1.4571923719406128\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[10/30] train_loss: 1.56543 valid_loss: 1.44243\n",
      "val_loss=1.4424288020133973\n",
      "Validation loss decreased (1.446366 --> 1.442429).  Saving model ...\n",
      "Updating learning rate to 5e-07\n",
      "[11/30] train_loss: 1.55737 valid_loss: 1.44817\n",
      "val_loss=1.4481738732337952\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[12/30] train_loss: 1.55977 valid_loss: 1.44386\n",
      "val_loss=1.4438568989753724\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[13/30] train_loss: 1.55422 valid_loss: 1.44768\n",
      "val_loss=1.4476841268539429\n",
      "EarlyStopping counter: 3 out of 4\n",
      "[14/30] train_loss: 1.55820 valid_loss: 1.44083\n",
      "val_loss=1.440831568145752\n",
      "Validation loss decreased (1.442429 --> 1.440832).  Saving model ...\n",
      "[15/30] train_loss: 1.55884 valid_loss: 1.44216\n",
      "val_loss=1.44215848903656\n",
      "EarlyStopping counter: 1 out of 4\n",
      "Updating learning rate to 1e-07\n",
      "[16/30] train_loss: 1.55855 valid_loss: 1.43608\n",
      "val_loss=1.4360786940574646\n",
      "Validation loss decreased (1.440832 --> 1.436079).  Saving model ...\n",
      "[17/30] train_loss: 1.55889 valid_loss: 1.44336\n",
      "val_loss=1.4433604483604432\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[18/30] train_loss: 1.55502 valid_loss: 1.44266\n",
      "val_loss=1.442662866783142\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[19/30] train_loss: 1.55171 valid_loss: 1.44647\n",
      "val_loss=1.4464718430519103\n",
      "EarlyStopping counter: 3 out of 4\n",
      "[20/30] train_loss: 1.55188 valid_loss: 1.44424\n",
      "val_loss=1.4442361931800842\n",
      "EarlyStopping counter: 4 out of 4\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "#加载数据集Dataset,DataLoader\n",
    "# 定义对数据的预处理\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # 转为Tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化\n",
    "                             ])\n",
    "transform_train = transforms.Compose([\n",
    "         # 对原始32*32图像四周各填充4个0像素（40*40），然后随机裁剪成32*32\n",
    "         transforms.RandomCrop(32, padding=4),\n",
    " \n",
    "         # 按0.5的概率水平翻转图片\n",
    "         transforms.RandomHorizontalFlip(),\n",
    " \n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "\n",
    "trainset = tv.datasets.ImageFolder(root='./data/cifar/train',transform=transform_train)\n",
    "\n",
    "trainloader = t.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = tv.datasets.ImageFolder(root='./data/cifar/test',  transform=transform)\n",
    "\n",
    "testloader = t.utils.data.DataLoader(testset, batch_size=args.batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "# 对图片random crop&flip\n",
    "model, train_losses, valid_losses = train_model(net, args.batch_size, args.patience, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000张测试集中的准确率为: 46 %\n"
     ]
    }
   ],
   "source": [
    "# random crop&flip\n",
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/30] train_loss: 1.95683 valid_loss: 2.36525\n",
      "val_loss=2.3652473142147064\n",
      "Validation loss decreased (inf --> 2.365247).  Saving model ...\n",
      "[ 2/30] train_loss: 1.85759 valid_loss: 1.61252\n",
      "val_loss=1.612519322013855\n",
      "Validation loss decreased (2.365247 --> 1.612519).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "[ 3/30] train_loss: 1.62085 valid_loss: 1.47012\n",
      "val_loss=1.4701238010406494\n",
      "Validation loss decreased (1.612519 --> 1.470124).  Saving model ...\n",
      "[ 4/30] train_loss: 1.56600 valid_loss: 1.45538\n",
      "val_loss=1.4553839234352113\n",
      "Validation loss decreased (1.470124 --> 1.455384).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "[ 5/30] train_loss: 1.54659 valid_loss: 1.41573\n",
      "val_loss=1.415728151988983\n",
      "Validation loss decreased (1.455384 --> 1.415728).  Saving model ...\n",
      "[ 6/30] train_loss: 1.52637 valid_loss: 1.41449\n",
      "val_loss=1.4144929316520691\n",
      "Validation loss decreased (1.415728 --> 1.414493).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "[ 7/30] train_loss: 1.51902 valid_loss: 1.40077\n",
      "val_loss=1.4007724413871765\n",
      "Validation loss decreased (1.414493 --> 1.400772).  Saving model ...\n",
      "[ 8/30] train_loss: 1.51751 valid_loss: 1.41420\n",
      "val_loss=1.4142004380702973\n",
      "EarlyStopping counter: 1 out of 4\n",
      "Updating learning rate to 1e-06\n",
      "[ 9/30] train_loss: 1.49666 valid_loss: 1.37852\n",
      "val_loss=1.3785210397720338\n",
      "Validation loss decreased (1.400772 --> 1.378521).  Saving model ...\n",
      "[10/30] train_loss: 1.49772 valid_loss: 1.39359\n",
      "val_loss=1.3935898503780364\n",
      "EarlyStopping counter: 1 out of 4\n",
      "Updating learning rate to 5e-07\n",
      "[11/30] train_loss: 1.50376 valid_loss: 1.37844\n",
      "val_loss=1.3784442271232604\n",
      "Validation loss decreased (1.378521 --> 1.378444).  Saving model ...\n",
      "[12/30] train_loss: 1.49989 valid_loss: 1.40923\n",
      "val_loss=1.4092256412506103\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[13/30] train_loss: 1.51226 valid_loss: 1.38514\n",
      "val_loss=1.3851365012168884\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[14/30] train_loss: 1.50937 valid_loss: 1.39312\n",
      "val_loss=1.3931205033779144\n",
      "EarlyStopping counter: 3 out of 4\n",
      "[15/30] train_loss: 1.51061 valid_loss: 1.39583\n",
      "val_loss=1.3958277604579925\n",
      "EarlyStopping counter: 4 out of 4\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "#加载数据集Dataset,DataLoader\n",
    "# 定义对数据的预处理\n",
    "import PIL.Image as Image\n",
    "class Cutout(object):\n",
    "     def __init__(self, hole_size):\n",
    "         # 正方形马赛克的边长，像素为单位\n",
    "         self.hole_size = hole_size\n",
    " \n",
    "     def __call__(self, img):\n",
    "         return cutout(img, self.hole_size)\n",
    " \n",
    "def cutout(img, hole_size):\n",
    "     y = np.random.randint(32)\n",
    "     x = np.random.randint(32)\n",
    " \n",
    "     half_size = hole_size // 2\n",
    " \n",
    "     x1 = np.clip(x - half_size, 0, 32)\n",
    "     x2 = np.clip(x + half_size, 0, 32)\n",
    "     y1 = np.clip(y - half_size, 0, 32)\n",
    "     y2 = np.clip(y + half_size, 0, 32)\n",
    " \n",
    "     imgnp = np.array(img)\n",
    " \n",
    "     imgnp[y1:y2, x1:x2] = 0\n",
    "     img = Image.fromarray(imgnp.astype('uint8')).convert('RGB')\n",
    "     return img\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "         # 对原始32*32图像四周各填充4个0像素（40*40），然后随机裁剪成32*32\n",
    "         transforms.RandomCrop(32, padding=4),\n",
    " \n",
    "         # 随机马赛克，大小为6*6\n",
    "         Cutout(6),\n",
    " \n",
    "         # 按0.5的概率水平翻转图片\n",
    "         transforms.RandomHorizontalFlip(),\n",
    " \n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "\n",
    "trainset = tv.datasets.ImageFolder(root='./data/cifar/train',transform=transform_train)\n",
    "\n",
    "trainloader = t.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "# cutout\n",
    "model, train_losses, valid_losses = train_model(net, args.batch_size, args.patience, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000张测试集中的准确率为: 47 %\n"
     ]
    }
   ],
   "source": [
    "# cutout\n",
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/30] train_loss: 2.21654 valid_loss: 3.03949\n",
      "val_loss=3.0394861227989196\n",
      "Validation loss decreased (inf --> 3.039486).  Saving model ...\n",
      "[ 2/30] train_loss: 2.08533 valid_loss: 1.91787\n",
      "val_loss=1.9178737157821655\n",
      "Validation loss decreased (3.039486 --> 1.917874).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "[ 3/30] train_loss: 1.86720 valid_loss: 1.46836\n",
      "val_loss=1.4683603600502013\n",
      "Validation loss decreased (1.917874 --> 1.468360).  Saving model ...\n",
      "[ 4/30] train_loss: 1.81452 valid_loss: 1.38494\n",
      "val_loss=1.3849391384124756\n",
      "Validation loss decreased (1.468360 --> 1.384939).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "[ 5/30] train_loss: 1.77805 valid_loss: 1.35308\n",
      "val_loss=1.3530834151268005\n",
      "Validation loss decreased (1.384939 --> 1.353083).  Saving model ...\n",
      "[ 6/30] train_loss: 1.76704 valid_loss: 1.36777\n",
      "val_loss=1.3677742599487304\n",
      "EarlyStopping counter: 1 out of 4\n",
      "Updating learning rate to 5e-06\n",
      "[ 7/30] train_loss: 1.76533 valid_loss: 1.35803\n",
      "val_loss=1.3580314249992371\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[ 8/30] train_loss: 1.76232 valid_loss: 1.34462\n",
      "val_loss=1.344616750907898\n",
      "Validation loss decreased (1.353083 --> 1.344617).  Saving model ...\n",
      "Updating learning rate to 1e-06\n",
      "[ 9/30] train_loss: 1.75837 valid_loss: 1.34015\n",
      "val_loss=1.3401492043495178\n",
      "Validation loss decreased (1.344617 --> 1.340149).  Saving model ...\n",
      "[10/30] train_loss: 1.75009 valid_loss: 1.33487\n",
      "val_loss=1.3348744270324706\n",
      "Validation loss decreased (1.340149 --> 1.334874).  Saving model ...\n",
      "Updating learning rate to 5e-07\n",
      "[11/30] train_loss: 1.74750 valid_loss: 1.38324\n",
      "val_loss=1.3832445956230164\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[12/30] train_loss: 1.74199 valid_loss: 1.34595\n",
      "val_loss=1.3459502801895142\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[13/30] train_loss: 1.76171 valid_loss: 1.33257\n",
      "val_loss=1.3325659062862396\n",
      "Validation loss decreased (1.334874 --> 1.332566).  Saving model ...\n",
      "[14/30] train_loss: 1.75270 valid_loss: 1.33961\n",
      "val_loss=1.3396118203163148\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[15/30] train_loss: 1.78063 valid_loss: 1.33017\n",
      "val_loss=1.3301709314346313\n",
      "Validation loss decreased (1.332566 --> 1.330171).  Saving model ...\n",
      "Updating learning rate to 1e-07\n",
      "[16/30] train_loss: 1.75699 valid_loss: 1.32536\n",
      "val_loss=1.325357096195221\n",
      "Validation loss decreased (1.330171 --> 1.325357).  Saving model ...\n",
      "[17/30] train_loss: 1.74910 valid_loss: 1.33103\n",
      "val_loss=1.3310299960136414\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[18/30] train_loss: 1.74051 valid_loss: 1.35523\n",
      "val_loss=1.3552282985687256\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[19/30] train_loss: 1.74181 valid_loss: 1.32158\n",
      "val_loss=1.321575733089447\n",
      "Validation loss decreased (1.325357 --> 1.321576).  Saving model ...\n",
      "[20/30] train_loss: 1.73935 valid_loss: 1.33257\n",
      "val_loss=1.3325734863758087\n",
      "EarlyStopping counter: 1 out of 4\n",
      "Updating learning rate to 5e-08\n",
      "[21/30] train_loss: 1.74162 valid_loss: 1.35096\n",
      "val_loss=1.350964737701416\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[22/30] train_loss: 1.76744 valid_loss: 1.31541\n",
      "val_loss=1.3154144909858703\n",
      "Validation loss decreased (1.321576 --> 1.315414).  Saving model ...\n",
      "[23/30] train_loss: 1.73716 valid_loss: 1.33670\n",
      "val_loss=1.3366988735198975\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[24/30] train_loss: 1.74355 valid_loss: 1.32495\n",
      "val_loss=1.3249542001247405\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[25/30] train_loss: 1.75462 valid_loss: 1.32129\n",
      "val_loss=1.3212922386169434\n",
      "EarlyStopping counter: 3 out of 4\n",
      "[26/30] train_loss: 1.75182 valid_loss: 1.35635\n",
      "val_loss=1.356350956058502\n",
      "EarlyStopping counter: 4 out of 4\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = t.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = t.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "#训练模型\n",
    "def train_model(model, batch_size, patience, n_epochs):\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # 训练模式\n",
    "        for batch, (data, target) in enumerate(trainloader, 1):\n",
    "            # 把数据存到GPU上\n",
    "            data = data.to(args.device)\n",
    "            target = target.to(args.device)\n",
    "            data, targets_a, targets_b, lam = mixup_data(data, target,\n",
    "                                                   1.)\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            # 计算loss\n",
    "            loss = mixup_criterion(criterion, output, targets_a, targets_b, lam)\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            # 记录训练loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # 切换到验证模式\n",
    "        for data, target in testloader:\n",
    "            # 把数据存到GPU上\n",
    "            data = data.to(args.device)\n",
    "            target = target.to(args.device)\n",
    "            data, targets_a, targets_b, lam = mixup_data(data, target,\n",
    "                                                   1.)\n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            # 计算loss\n",
    "            loss = mixup_criterion(criterion, output, targets_a, targets_b, lam)\n",
    "            # 记录验证loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # 计算平均loss\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # 下个epoch前清空loss\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        #==================early stopping======================\n",
    "        early_stopping(avg_valid_losses[-1],model=model,path='model')\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        #====================adjust lr========================\n",
    "        lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    return  model, avg_train_losses, avg_valid_losses\n",
    "\n",
    "# mixup\n",
    "model, train_losses, valid_losses = train_model(net, args.batch_size, args.patience, args.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000张测试集中的准确率为: 49 %\n"
     ]
    }
   ],
   "source": [
    "# mixup\n",
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/30] train_loss: 2.06623 valid_loss: 2.23897\n",
      "val_loss=2.2389658091545104\n",
      "Validation loss decreased (inf --> 2.238966).  Saving model ...\n",
      "[ 2/30] train_loss: 1.91958 valid_loss: 1.87121\n",
      "val_loss=1.87121098279953\n",
      "Validation loss decreased (2.238966 --> 1.871211).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "[ 3/30] train_loss: 1.75061 valid_loss: 1.42077\n",
      "val_loss=1.4207736505508424\n",
      "Validation loss decreased (1.871211 --> 1.420774).  Saving model ...\n",
      "[ 4/30] train_loss: 1.67514 valid_loss: 1.34185\n",
      "val_loss=1.34185367269516\n",
      "Validation loss decreased (1.420774 --> 1.341854).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "[ 5/30] train_loss: 1.62610 valid_loss: 1.32748\n",
      "val_loss=1.3274779858589172\n",
      "Validation loss decreased (1.341854 --> 1.327478).  Saving model ...\n",
      "[ 6/30] train_loss: 1.59506 valid_loss: 1.30771\n",
      "val_loss=1.307708592605591\n",
      "Validation loss decreased (1.327478 --> 1.307709).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "[ 7/30] train_loss: 1.57056 valid_loss: 1.30726\n",
      "val_loss=1.3072552298545836\n",
      "Validation loss decreased (1.307709 --> 1.307255).  Saving model ...\n",
      "[ 8/30] train_loss: 1.59889 valid_loss: 1.29263\n",
      "val_loss=1.2926304271697997\n",
      "Validation loss decreased (1.307255 --> 1.292630).  Saving model ...\n",
      "Updating learning rate to 1e-06\n",
      "[ 9/30] train_loss: 1.56888 valid_loss: 1.29466\n",
      "val_loss=1.2946634320259094\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[10/30] train_loss: 1.58140 valid_loss: 1.29289\n",
      "val_loss=1.2928855583667755\n",
      "EarlyStopping counter: 2 out of 4\n",
      "Updating learning rate to 5e-07\n",
      "[11/30] train_loss: 1.57360 valid_loss: 1.30252\n",
      "val_loss=1.3025225120544432\n",
      "EarlyStopping counter: 3 out of 4\n",
      "[12/30] train_loss: 1.57075 valid_loss: 1.28999\n",
      "val_loss=1.2899884349822999\n",
      "Validation loss decreased (1.292630 --> 1.289988).  Saving model ...\n",
      "[13/30] train_loss: 1.58106 valid_loss: 1.29775\n",
      "val_loss=1.2977536045074463\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[14/30] train_loss: 1.55564 valid_loss: 1.30382\n",
      "val_loss=1.3038187460899353\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[15/30] train_loss: 1.56798 valid_loss: 1.28702\n",
      "val_loss=1.2870213537216186\n",
      "Validation loss decreased (1.289988 --> 1.287021).  Saving model ...\n",
      "Updating learning rate to 1e-07\n",
      "[16/30] train_loss: 1.56720 valid_loss: 1.29186\n",
      "val_loss=1.2918607545375824\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[17/30] train_loss: 1.56807 valid_loss: 1.29189\n",
      "val_loss=1.2918897203445434\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[18/30] train_loss: 1.56675 valid_loss: 1.28874\n",
      "val_loss=1.288736016559601\n",
      "EarlyStopping counter: 3 out of 4\n",
      "[19/30] train_loss: 1.57180 valid_loss: 1.29355\n",
      "val_loss=1.2935534026145934\n",
      "EarlyStopping counter: 4 out of 4\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "def shuffle_minibatch(x, y):\n",
    "    assert x.size(0)== y.size(0)\n",
    "    indices = t.randperm(x.size(0))\n",
    "    return x[indices], y[indices]\n",
    "#训练模型\n",
    "def train_model(model, batch_size, patience, n_epochs):\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # 训练模式\n",
    "        for batch, (data, target) in enumerate(trainloader, 1):\n",
    "            # 把数据存到GPU上\n",
    "            data = data.to(args.device)\n",
    "            target = target.to(args.device)\n",
    "            cutmix_decision = np.random.rand()\n",
    "            if cutmix_decision > 0.50:\n",
    "                # Cutmix: https://arxiv.org/pdf/1905.04899.pdf\n",
    "                x_train_shuffled, y_train_shuffled = shuffle_minibatch(data, target)\n",
    "                lam = np.random.beta(1, 1)\n",
    "                cut_rat = np.sqrt(1. - lam)\n",
    "                cut_w = np.int64(32 * cut_rat)\n",
    "                cut_h = np.int64(32 * cut_rat)\n",
    "\n",
    "                # uniform\n",
    "                cx = np.random.randint(32)\n",
    "                cy = np.random.randint(32)\n",
    "\n",
    "                bbx1 = np.clip(cx - cut_w // 2, 0, 32)\n",
    "                bby1 = np.clip(cy - cut_h // 2, 0, 32)\n",
    "                bbx2 = np.clip(cx + cut_w // 2, 0, 32)\n",
    "                bby2 = np.clip(cy + cut_h // 2, 0, 32)\n",
    "\n",
    "                data[:, :, bbx1:bbx2, bby1:bby2] = x_train_shuffled[:, :, bbx1:bbx2, bby1:bby2]\n",
    "                lam = 1 - (bbx2 - bbx1) * (bby2 - bby1) / (32 * 32)\n",
    "        \n",
    "            # Forward pass\n",
    "            y_preds = model(data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            if cutmix_decision > 0.50:\n",
    "                loss = criterion(y_preds, target) * lam + criterion(y_preds, y_train_shuffled) * (1. - lam)\n",
    "            else:\n",
    "                loss = criterion(y_preds, target)\n",
    "            \n",
    "            \n",
    "            # Backpropagate\n",
    "            optimizer.zero_grad()  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # 切换到验证模式\n",
    "        for data, target in testloader:\n",
    "            # 把数据存到GPU上\n",
    "            data = data.to(args.device)\n",
    "            target = target.to(args.device)\n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            # 计算loss\n",
    "            loss = criterion(output, target)\n",
    "            # 记录验证loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # 计算平均loss\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # 下个epoch前清空loss\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        #==================early stopping======================\n",
    "        early_stopping(avg_valid_losses[-1],model=model,path='model')\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        #====================adjust lr========================\n",
    "        lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    return  model, avg_train_losses, avg_valid_losses\n",
    "\n",
    "\n",
    "# cutmix\n",
    "model, train_losses, valid_losses = train_model(net, args.batch_size, args.patience, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000张测试集中的准确率为: 53 %\n"
     ]
    }
   ],
   "source": [
    "# cutmix\n",
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type   | Params\n",
      "--------------------------------\n",
      "0 | base | ResNet | 21.3 M\n",
      "--------------------------------\n",
      "21.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "21.3 M    Total params\n",
      "85.159    Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 48 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1250/1250 [00:40<00:00, 31.05it/s, loss=2.08, v_num=3]Epoch:0 | Loss:2.0343804359436035 | Accuracy:0.2142\n",
      "Epoch 1: 100%|██████████| 1250/1250 [00:39<00:00, 31.43it/s, loss=1.99, v_num=3]Epoch:1 | Loss:1.8775088787078857 | Accuracy:0.2805\n",
      "Epoch 2: 100%|██████████| 1250/1250 [00:39<00:00, 31.79it/s, loss=1.77, v_num=3]Epoch:2 | Loss:1.7517582178115845 | Accuracy:0.3299\n",
      "Epoch 3: 100%|██████████| 1250/1250 [00:40<00:00, 30.92it/s, loss=1.77, v_num=3]Epoch:3 | Loss:1.6618013381958008 | Accuracy:0.3766\n",
      "Epoch 4: 100%|██████████| 1250/1250 [00:40<00:00, 30.89it/s, loss=1.64, v_num=3]Epoch:4 | Loss:1.561708927154541 | Accuracy:0.4249\n",
      "Epoch 5: 100%|██████████| 1250/1250 [00:39<00:00, 31.38it/s, loss=1.52, v_num=3]Epoch:5 | Loss:1.5043737888336182 | Accuracy:0.4565\n",
      "Epoch 6: 100%|██████████| 1250/1250 [00:39<00:00, 31.90it/s, loss=1.29, v_num=3]Epoch:6 | Loss:1.3936092853546143 | Accuracy:0.4945\n",
      "Epoch 7: 100%|██████████| 1250/1250 [00:38<00:00, 32.49it/s, loss=1.28, v_num=3]Epoch:7 | Loss:1.306585431098938 | Accuracy:0.5308\n",
      "Epoch 8: 100%|██████████| 1250/1250 [00:39<00:00, 31.28it/s, loss=1.2, v_num=3] Epoch:8 | Loss:1.4209340810775757 | Accuracy:0.5142\n",
      "Epoch 9: 100%|██████████| 1250/1250 [00:39<00:00, 31.29it/s, loss=1.01, v_num=3]Epoch:9 | Loss:1.2407796382904053 | Accuracy:0.5649\n",
      "Epoch 10: 100%|██████████| 1250/1250 [00:41<00:00, 30.09it/s, loss=0.949, v_num=3]Epoch:10 | Loss:1.26903235912323 | Accuracy:0.5649\n",
      "Epoch 11: 100%|██████████| 1250/1250 [00:40<00:00, 30.99it/s, loss=0.68, v_num=3] Epoch:11 | Loss:1.3545013666152954 | Accuracy:0.5822\n",
      "Epoch 12: 100%|██████████| 1250/1250 [00:40<00:00, 31.08it/s, loss=0.644, v_num=3]Epoch:12 | Loss:1.360754132270813 | Accuracy:0.5803\n",
      "Epoch 13: 100%|██████████| 1250/1250 [00:40<00:00, 30.79it/s, loss=0.564, v_num=3]Epoch:13 | Loss:1.3968671560287476 | Accuracy:0.5754\n",
      "Epoch 14: 100%|██████████| 1250/1250 [00:40<00:00, 31.23it/s, loss=0.341, v_num=3]Epoch:14 | Loss:1.5207631587982178 | Accuracy:0.5944\n",
      "Epoch 15: 100%|██████████| 1250/1250 [00:40<00:00, 31.14it/s, loss=0.359, v_num=3]Epoch:15 | Loss:1.7557581663131714 | Accuracy:0.5824\n",
      "Epoch 16: 100%|██████████| 1250/1250 [00:40<00:00, 30.64it/s, loss=0.274, v_num=3]Epoch:16 | Loss:1.808553695678711 | Accuracy:0.579\n",
      "Epoch 17: 100%|██████████| 1250/1250 [00:40<00:00, 30.98it/s, loss=0.324, v_num=3]Epoch:17 | Loss:1.8037495613098145 | Accuracy:0.5689\n",
      "Epoch 18: 100%|██████████| 1250/1250 [00:40<00:00, 30.78it/s, loss=0.242, v_num=3]Epoch:18 | Loss:2.0638959407806396 | Accuracy:0.5706\n",
      "Epoch 19: 100%|██████████| 1250/1250 [00:40<00:00, 30.56it/s, loss=0.112, v_num=3]Epoch:19 | Loss:2.0307729244232178 | Accuracy:0.584\n",
      "Epoch 20: 100%|██████████| 1250/1250 [00:39<00:00, 31.47it/s, loss=0.229, v_num=3]Epoch:20 | Loss:2.098917007446289 | Accuracy:0.5751\n",
      "Epoch 21: 100%|██████████| 1250/1250 [00:39<00:00, 31.36it/s, loss=0.215, v_num=3]Epoch:21 | Loss:2.0579917430877686 | Accuracy:0.5796\n",
      "Epoch 22: 100%|██████████| 1250/1250 [00:40<00:00, 30.53it/s, loss=0.121, v_num=3]Epoch:22 | Loss:2.3093695640563965 | Accuracy:0.5822\n",
      "Epoch 23: 100%|██████████| 1250/1250 [00:39<00:00, 31.35it/s, loss=0.15, v_num=3] Epoch:23 | Loss:2.054379463195801 | Accuracy:0.5844\n",
      "Epoch 24: 100%|██████████| 1250/1250 [00:41<00:00, 30.30it/s, loss=0.156, v_num=3]Epoch:24 | Loss:2.3326616287231445 | Accuracy:0.5635\n",
      "Epoch 25: 100%|██████████| 1250/1250 [00:40<00:00, 30.73it/s, loss=0.0859, v_num=3]Epoch:25 | Loss:2.3599464893341064 | Accuracy:0.5886\n",
      "Epoch 26: 100%|██████████| 1250/1250 [00:41<00:00, 30.30it/s, loss=0.124, v_num=3] Epoch:26 | Loss:2.3420188426971436 | Accuracy:0.5806\n",
      "Epoch 27: 100%|██████████| 1250/1250 [00:41<00:00, 30.18it/s, loss=0.0673, v_num=3]Epoch:27 | Loss:2.4190542697906494 | Accuracy:0.5875\n",
      "Epoch 28: 100%|██████████| 1250/1250 [00:40<00:00, 31.12it/s, loss=0.0863, v_num=3]Epoch:28 | Loss:2.3152124881744385 | Accuracy:0.5905\n",
      "Epoch 29: 100%|██████████| 1250/1250 [00:41<00:00, 30.44it/s, loss=0.116, v_num=3] Epoch:29 | Loss:2.176612377166748 | Accuracy:0.5788\n",
      "Epoch 29: 100%|██████████| 1250/1250 [00:41<00:00, 30.01it/s, loss=0.116, v_num=3]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "class GroupNorm_32(t.nn.GroupNorm):\n",
    "    def __init__(self, num_channels, num_groups=32, **kwargs):\n",
    "        super().__init__(num_groups, num_channels, **kwargs)\n",
    "resnet34_gn = models.resnet34(norm_layer=GroupNorm_32, num_classes=10)\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "class Model(LightningModule):\n",
    "    def __init__(self, base):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return t.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "    def step(self, batch):\n",
    "        x, y  = batch\n",
    "        y_hat = self(x)\n",
    "        loss  = nn.CrossEntropyLoss()(y_hat, y)\n",
    "        return loss, y, y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        loss, _, _ = self.step(batch)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        return {'loss': loss, 'y': y.detach(), 'y_hat': y_hat.detach()}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = t.stack([x['loss'] for x in outputs]).mean()\n",
    "        acc = self.get_accuracy(outputs)\n",
    "        print(f\"Epoch:{self.current_epoch} | Loss:{avg_loss} | Accuracy:{acc}\")\n",
    "        return {'loss': avg_loss}\n",
    "    \n",
    "    def get_accuracy(self, outputs):\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        y = t.cat([x['y'] for x in outputs])\n",
    "        y_hat = t.cat([x['y_hat'] for x in outputs])\n",
    "        preds = y_hat.argmax(1)\n",
    "        return accuracy_score(y.cpu().numpy(), preds.cpu().numpy())\n",
    "model_gn = Model(resnet34_gn).to(args.device)\n",
    "debug = False\n",
    "trainer = Trainer(gpus=1, max_epochs=30, \n",
    "                  num_sanity_val_steps=1 if debug else 0)\n",
    "\n",
    "\n",
    "# 将resnet34_bn模型转换为resnet34_gn模型\n",
    "trainer.fit(model_gn,trainloader,testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(model_gn, 'resnet34_gn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/30] train_loss: 2.16646 valid_loss: 1.93164\n",
      "val_loss=1.9316405723571777\n",
      "Validation loss decreased (inf --> 1.931641).  Saving model ...\n",
      "[ 2/30] train_loss: 1.89352 valid_loss: 1.96421\n",
      "val_loss=1.9642088680267333\n",
      "EarlyStopping counter: 1 out of 4\n",
      "Updating learning rate to 5e-05\n",
      "[ 3/30] train_loss: 1.59027 valid_loss: 1.49895\n",
      "val_loss=1.4989541625022889\n",
      "Validation loss decreased (1.931641 --> 1.498954).  Saving model ...\n",
      "[ 4/30] train_loss: 1.50472 valid_loss: 1.45122\n",
      "val_loss=1.4512166882514954\n",
      "Validation loss decreased (1.498954 --> 1.451217).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "[ 5/30] train_loss: 1.46673 valid_loss: 1.45484\n",
      "val_loss=1.4548353902816773\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[ 6/30] train_loss: 1.45509 valid_loss: 1.49617\n",
      "val_loss=1.4961650044441224\n",
      "EarlyStopping counter: 2 out of 4\n",
      "Updating learning rate to 5e-06\n",
      "[ 7/30] train_loss: 1.43581 valid_loss: 1.50623\n",
      "val_loss=1.5062320848464965\n",
      "EarlyStopping counter: 3 out of 4\n",
      "[ 8/30] train_loss: 1.44837 valid_loss: 1.45067\n",
      "val_loss=1.4506748849868774\n",
      "Validation loss decreased (1.451217 --> 1.450675).  Saving model ...\n",
      "Updating learning rate to 1e-06\n",
      "[ 9/30] train_loss: 1.43597 valid_loss: 1.43716\n",
      "val_loss=1.4371588025093078\n",
      "Validation loss decreased (1.450675 --> 1.437159).  Saving model ...\n",
      "[10/30] train_loss: 1.43473 valid_loss: 1.42180\n",
      "val_loss=1.4218048735618591\n",
      "Validation loss decreased (1.437159 --> 1.421805).  Saving model ...\n",
      "Updating learning rate to 5e-07\n",
      "[11/30] train_loss: 1.44021 valid_loss: 1.44741\n",
      "val_loss=1.4474062225341797\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[12/30] train_loss: 1.44010 valid_loss: 1.47353\n",
      "val_loss=1.4735255663871765\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[13/30] train_loss: 1.43199 valid_loss: 1.41290\n",
      "val_loss=1.412904118824005\n",
      "Validation loss decreased (1.421805 --> 1.412904).  Saving model ...\n",
      "[14/30] train_loss: 1.43294 valid_loss: 1.47179\n",
      "val_loss=1.4717869568824768\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[15/30] train_loss: 1.44113 valid_loss: 1.42949\n",
      "val_loss=1.4294907470703124\n",
      "EarlyStopping counter: 2 out of 4\n",
      "Updating learning rate to 1e-07\n",
      "[16/30] train_loss: 1.43876 valid_loss: 1.44686\n",
      "val_loss=1.446855401992798\n",
      "EarlyStopping counter: 3 out of 4\n",
      "[17/30] train_loss: 1.43048 valid_loss: 1.43854\n",
      "val_loss=1.4385391542434693\n",
      "EarlyStopping counter: 4 out of 4\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "def train_model(model, batch_size, patience, n_epochs):\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # 训练模式\n",
    "        for batch, (data, target) in enumerate(trainloader, 1):\n",
    "            # 把数据存到GPU上\n",
    "            data = data.to(args.device)\n",
    "            target = target.to(args.device)\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            # 计算loss\n",
    "            loss = criterion(output, target)\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 梯度裁剪\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            # 记录训练loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # 切换到验证模式\n",
    "        for data, target in testloader:\n",
    "            # 把数据存到GPU上\n",
    "            data = data.to(args.device)\n",
    "            target = target.to(args.device)\n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            # 计算loss\n",
    "            loss = criterion(output, target)\n",
    "            # 记录验证loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # 计算平均loss\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # 下个epoch前清空loss\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        #==================early stopping======================\n",
    "        early_stopping(avg_valid_losses[-1],model=model,path='model')\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        #====================adjust lr========================\n",
    "        lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    return  model, avg_train_losses, avg_valid_losses\n",
    "\n",
    "# 梯度裁剪\n",
    "model, train_losses, valid_losses = train_model(net, args.batch_size, args.patience, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000张测试集中的准确率为: 47 %\n"
     ]
    }
   ],
   "source": [
    "# 梯度裁剪\n",
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/30] train_loss: 2.10855 valid_loss: 1.88664\n",
      "val_loss=1.8866373000144958\n",
      "Validation loss decreased (inf --> 1.886637).  Saving model ...\n",
      "[ 2/30] train_loss: 1.74421 valid_loss: 1.58326\n",
      "val_loss=1.5832586451530457\n",
      "Validation loss decreased (1.886637 --> 1.583259).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "[ 3/30] train_loss: 1.41282 valid_loss: 1.45728\n",
      "val_loss=1.4572807833194732\n",
      "Validation loss decreased (1.583259 --> 1.457281).  Saving model ...\n",
      "[ 4/30] train_loss: 1.34946 valid_loss: 1.42323\n",
      "val_loss=1.423228444480896\n",
      "Validation loss decreased (1.457281 --> 1.423228).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "[ 5/30] train_loss: 1.28969 valid_loss: 1.42697\n",
      "val_loss=1.4269720645427704\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[ 6/30] train_loss: 1.30480 valid_loss: 1.41963\n",
      "val_loss=1.4196281918525695\n",
      "Validation loss decreased (1.423228 --> 1.419628).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "[ 7/30] train_loss: 1.28283 valid_loss: 1.41378\n",
      "val_loss=1.4137841077327729\n",
      "Validation loss decreased (1.419628 --> 1.413784).  Saving model ...\n",
      "[ 8/30] train_loss: 1.28667 valid_loss: 1.41367\n",
      "val_loss=1.4136748045921326\n",
      "Validation loss decreased (1.413784 --> 1.413675).  Saving model ...\n",
      "Updating learning rate to 1e-06\n",
      "[ 9/30] train_loss: 1.27806 valid_loss: 1.41894\n",
      "val_loss=1.4189408762454987\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[10/30] train_loss: 1.26895 valid_loss: 1.40889\n",
      "val_loss=1.4088852296829224\n",
      "Validation loss decreased (1.413675 --> 1.408885).  Saving model ...\n",
      "Updating learning rate to 5e-07\n",
      "[11/30] train_loss: 1.26589 valid_loss: 1.41473\n",
      "val_loss=1.41472708568573\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[12/30] train_loss: 1.26947 valid_loss: 1.40512\n",
      "val_loss=1.4051168712615967\n",
      "Validation loss decreased (1.408885 --> 1.405117).  Saving model ...\n",
      "[13/30] train_loss: 1.26152 valid_loss: 1.41124\n",
      "val_loss=1.4112418799877167\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[14/30] train_loss: 1.27095 valid_loss: 1.41259\n",
      "val_loss=1.4125915086269378\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[15/30] train_loss: 1.26975 valid_loss: 1.41018\n",
      "val_loss=1.4101765709877014\n",
      "EarlyStopping counter: 3 out of 4\n",
      "Updating learning rate to 1e-07\n",
      "[16/30] train_loss: 1.27576 valid_loss: 1.41005\n",
      "val_loss=1.4100529753684998\n",
      "EarlyStopping counter: 4 out of 4\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "def l2_penalty(w):\n",
    "    return t.sum(w.pow(2)) / 2\n",
    "#训练模型\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.learning_rate, momentum=0.9, weight_decay=5e-4) #优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "def train_model(model, batch_size, patience, n_epochs):\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # 训练模式\n",
    "        for batch, (data, target) in enumerate(trainloader, 1):\n",
    "            # 把数据存到GPU上\n",
    "            data = data.to(args.device)\n",
    "            target = target.to(args.device)\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            # 计算loss\n",
    "            loss = criterion(output, target)\n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "            # 记录训练loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # 切换到验证模式\n",
    "        for data, target in testloader:\n",
    "            # 把数据存到GPU上\n",
    "            data = data.to(args.device)\n",
    "            target = target.to(args.device)\n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            # 计算loss\n",
    "            loss = criterion(output, target)\n",
    "            # 记录验证loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # 计算平均loss\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "        print(print_msg)\n",
    "        \n",
    "        # 下个epoch前清空loss\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        #==================early stopping======================\n",
    "        early_stopping(avg_valid_losses[-1],model=model,path='model')\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        #====================adjust lr========================\n",
    "        lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    return  model, avg_train_losses, avg_valid_losses\n",
    "\n",
    "\n",
    "# 权重衰减\n",
    "model, train_losses, valid_losses = train_model(net, args.batch_size, args.patience, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000张测试集中的准确率为: 48 %\n"
     ]
    }
   ],
   "source": [
    "# 权重衰减\n",
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 7527\n",
      "    Root location: ./data/cifar/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               RandomCrop(size=(32, 32), padding=4)\n",
      "               RandomHorizontalFlip(p=0.5)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # 转为Tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化\n",
    "                             ])\n",
    "transform_train = transforms.Compose([\n",
    "         # 对原始32*32图像四周各填充4个0像素（40*40），然后随机裁剪成32*32\n",
    "         transforms.RandomCrop(32, padding=4),\n",
    " \n",
    "         # 按0.5的概率水平翻转图片\n",
    "         transforms.RandomHorizontalFlip(),\n",
    " \n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "\n",
    "trainset = tv.datasets.ImageFolder(root='./data/cifar/train',transform=transform_train)\n",
    "\n",
    "trainloader = t.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = tv.datasets.ImageFolder(root='./data/cifar/test',  transform=transform)\n",
    "\n",
    "testloader = t.utils.data.DataLoader(testset, batch_size=args.batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "print(trainset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/30] train_loss: 2.10590 valid_loss: 2.17729\n",
      "val_loss=2.177291615486145\n",
      "Validation loss decreased (inf --> 2.177292).  Saving model ...\n",
      "[ 2/30] train_loss: 1.93789 valid_loss: 2.40523\n",
      "val_loss=2.4052278933525084\n",
      "EarlyStopping counter: 1 out of 4\n",
      "Updating learning rate to 5e-05\n",
      "[ 3/30] train_loss: 1.68928 valid_loss: 1.53824\n",
      "val_loss=1.5382410376548767\n",
      "Validation loss decreased (2.177292 --> 1.538241).  Saving model ...\n",
      "[ 4/30] train_loss: 1.59865 valid_loss: 1.49411\n",
      "val_loss=1.4941063723564147\n",
      "Validation loss decreased (1.538241 --> 1.494106).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "[ 5/30] train_loss: 1.56614 valid_loss: 1.44868\n",
      "val_loss=1.4486836185455323\n",
      "Validation loss decreased (1.494106 --> 1.448684).  Saving model ...\n",
      "[ 6/30] train_loss: 1.55138 valid_loss: 1.45089\n",
      "val_loss=1.4508922484397888\n",
      "EarlyStopping counter: 1 out of 4\n",
      "Updating learning rate to 5e-06\n",
      "[ 7/30] train_loss: 1.52034 valid_loss: 1.44684\n",
      "val_loss=1.4468447987556456\n",
      "Validation loss decreased (1.448684 --> 1.446845).  Saving model ...\n",
      "[ 8/30] train_loss: 1.52058 valid_loss: 1.43041\n",
      "val_loss=1.4304059059143066\n",
      "Validation loss decreased (1.446845 --> 1.430406).  Saving model ...\n",
      "Updating learning rate to 1e-06\n",
      "[ 9/30] train_loss: 1.52023 valid_loss: 1.43549\n",
      "val_loss=1.435491168308258\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[10/30] train_loss: 1.51310 valid_loss: 1.46363\n",
      "val_loss=1.4636339593887329\n",
      "EarlyStopping counter: 2 out of 4\n",
      "Updating learning rate to 5e-07\n",
      "[11/30] train_loss: 1.51979 valid_loss: 1.45147\n",
      "val_loss=1.4514700728416443\n",
      "EarlyStopping counter: 3 out of 4\n",
      "[12/30] train_loss: 1.51609 valid_loss: 1.42400\n",
      "val_loss=1.4239997535705566\n",
      "Validation loss decreased (1.430406 --> 1.424000).  Saving model ...\n",
      "[13/30] train_loss: 1.52336 valid_loss: 1.43467\n",
      "val_loss=1.4346680722236633\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[14/30] train_loss: 1.50998 valid_loss: 1.42449\n",
      "val_loss=1.4244858763694763\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[15/30] train_loss: 1.51208 valid_loss: 1.43168\n",
      "val_loss=1.431680999469757\n",
      "EarlyStopping counter: 3 out of 4\n",
      "Updating learning rate to 1e-07\n",
      "[16/30] train_loss: 1.51386 valid_loss: 1.45872\n",
      "val_loss=1.4587222121238708\n",
      "EarlyStopping counter: 4 out of 4\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# 类别不均衡问题下Baseline\n",
    "model, train_losses, valid_losses = train_model(net, args.batch_size, args.patience, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000张测试集中的准确率为: 26 %\n"
     ]
    }
   ],
   "source": [
    "# 类别不均衡\n",
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_for_balanced_classes(images, nclasses):                        \n",
    "    count = [0] * nclasses                                                      \n",
    "    for item in images:                                                         \n",
    "        count[item[1]] += 1                                                     \n",
    "    weight_per_class = [0.] * nclasses                                      \n",
    "    N = float(sum(count))                                                   \n",
    "    for i in range(nclasses):                                                   \n",
    "        weight_per_class[i] = N/float(count[i])                                 \n",
    "    weight = [0] * len(images)                                              \n",
    "    for idx, val in enumerate(images):                                          \n",
    "        weight[idx] = weight_per_class[val[1]]                                  \n",
    "    return weight             \n",
    "import torch \n",
    "trainset = tv.datasets.ImageFolder(root='./data/cifar/train',transform=transform_train)                                                           \n",
    "                                                                                \n",
    "# For unbalanced dataset we create a weighted sampler                       \n",
    "weights = make_weights_for_balanced_classes(trainset.imgs, len(trainset.classes))                                                                \n",
    "weights = torch.DoubleTensor(weights)                                       \n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))                     \n",
    "                                                                                \n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,sampler = sampler, num_workers=2, pin_memory=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/30] train_loss: 2.10590 valid_loss: 2.17729\n",
      "val_loss=2.177291615486145\n",
      "Validation loss decreased (inf --> 2.177292).  Saving model ...\n",
      "[ 2/30] train_loss: 1.93789 valid_loss: 2.40523\n",
      "val_loss=2.4052278933525084\n",
      "EarlyStopping counter: 1 out of 4\n",
      "Updating learning rate to 5e-05\n",
      "[ 3/30] train_loss: 1.68928 valid_loss: 1.53824\n",
      "val_loss=1.5382410376548767\n",
      "Validation loss decreased (2.177292 --> 1.538241).  Saving model ...\n",
      "[ 4/30] train_loss: 1.59865 valid_loss: 1.49411\n",
      "val_loss=1.4941063723564147\n",
      "Validation loss decreased (1.538241 --> 1.494106).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "[ 5/30] train_loss: 1.56614 valid_loss: 1.44868\n",
      "val_loss=1.4486836185455323\n",
      "Validation loss decreased (1.494106 --> 1.448684).  Saving model ...\n",
      "[ 6/30] train_loss: 1.55138 valid_loss: 1.45089\n",
      "val_loss=1.4508922484397888\n",
      "EarlyStopping counter: 1 out of 4\n",
      "Updating learning rate to 5e-06\n",
      "[ 7/30] train_loss: 1.52034 valid_loss: 1.44684\n",
      "val_loss=1.4468447987556456\n",
      "Validation loss decreased (1.448684 --> 1.446845).  Saving model ...\n",
      "[ 8/30] train_loss: 1.52058 valid_loss: 1.43041\n",
      "val_loss=1.4304059059143066\n",
      "Validation loss decreased (1.446845 --> 1.430406).  Saving model ...\n",
      "Updating learning rate to 1e-06\n",
      "[ 9/30] train_loss: 1.52023 valid_loss: 1.43549\n",
      "val_loss=1.435491168308258\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[10/30] train_loss: 1.51310 valid_loss: 1.46363\n",
      "val_loss=1.4636339593887329\n",
      "EarlyStopping counter: 2 out of 4\n",
      "Updating learning rate to 5e-07\n",
      "[11/30] train_loss: 1.51979 valid_loss: 1.45147\n",
      "val_loss=1.4514700728416443\n",
      "EarlyStopping counter: 3 out of 4\n",
      "[12/30] train_loss: 1.51609 valid_loss: 1.42400\n",
      "val_loss=1.4239997535705566\n",
      "Validation loss decreased (1.430406 --> 1.424000).  Saving model ...\n",
      "[13/30] train_loss: 1.52336 valid_loss: 1.43467\n",
      "val_loss=1.4346680722236633\n",
      "EarlyStopping counter: 1 out of 4\n",
      "[14/30] train_loss: 1.50998 valid_loss: 1.42449\n",
      "val_loss=1.4244858763694763\n",
      "EarlyStopping counter: 2 out of 4\n",
      "[15/30] train_loss: 1.51208 valid_loss: 1.43168\n",
      "val_loss=1.431680999469757\n",
      "EarlyStopping counter: 3 out of 4\n",
      "Updating learning rate to 1e-07\n",
      "[16/30] train_loss: 1.51386 valid_loss: 1.45872\n",
      "val_loss=1.4587222121238708\n",
      "EarlyStopping counter: 4 out of 4\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# 通过权重随机过采样\n",
    "model, train_losses, valid_losses = train_model(net, args.batch_size, args.patience, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000张测试集中的准确率为: 46 %\n"
     ]
    }
   ],
   "source": [
    "# 类别不均衡过采样\n",
    "get_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  MixMatch-pytorch-master.zip\n",
      "   creating: MixMatch-pytorch-master/dataset/\n",
      "   creating: MixMatch-pytorch-master/dataset/__pycache__/\n",
      "  inflating: MixMatch-pytorch-master/dataset/__pycache__/cifar10.cpython-37.pyc  \n",
      "  inflating: MixMatch-pytorch-master/dataset/cifar10.py  \n",
      "  inflating: MixMatch-pytorch-master/LICENSE  \n",
      "   creating: MixMatch-pytorch-master/models/\n",
      "   creating: MixMatch-pytorch-master/models/__pycache__/\n",
      "  inflating: MixMatch-pytorch-master/models/__pycache__/wideresnet.cpython-37.pyc  \n",
      "  inflating: MixMatch-pytorch-master/models/wideresnet.py  \n",
      "  inflating: MixMatch-pytorch-master/README.md  \n",
      "  inflating: MixMatch-pytorch-master/train.py  \n",
      "   creating: MixMatch-pytorch-master/utils/\n",
      "  inflating: MixMatch-pytorch-master/utils/__init__.py  \n",
      "   creating: MixMatch-pytorch-master/utils/__pycache__/\n",
      "  inflating: MixMatch-pytorch-master/utils/__pycache__/__init__.cpython-37.pyc  \n",
      "  inflating: MixMatch-pytorch-master/utils/__pycache__/eval.cpython-37.pyc  \n",
      "  inflating: MixMatch-pytorch-master/utils/__pycache__/logger.cpython-37.pyc  \n",
      "  inflating: MixMatch-pytorch-master/utils/__pycache__/misc.cpython-37.pyc  \n",
      "  inflating: MixMatch-pytorch-master/utils/eval.py  \n",
      "  inflating: MixMatch-pytorch-master/utils/logger.py  \n",
      "  inflating: MixMatch-pytorch-master/utils/misc.py  \n"
     ]
    }
   ],
   "source": [
    "# 部分标注问题MixMatch\n",
    "!unzip MixMatch-pytorch-master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing cifar10\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
      "170499072it [00:59, 2877245.89it/s]                                             \n",
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "#Labeled: 250 #Unlabeled: 44750 #Val: 5000\n",
      "==> creating WRN-28-2\n",
      "    Total params: 1.47M\n",
      "\n",
      "Epoch: [1 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 1.1968 | Loss_x: 1.1963 | Loss_u: 0.0115 | W: 0.0366\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.013s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.9823 | top1:  35.9375 | top5:  74.4792\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 2.2552 | top1:  20.3600 | top5:  67.4800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 2.2411 | top1:  20.7600 | top5:  67.8400\n",
      "\n",
      "Epoch: [2 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.9025 | Loss_x: 0.9013 | Loss_u: 0.0114 | W: 0.1098\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.5430 | top1:  95.3125 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.7628 | top1:  38.3600 | top5:  87.4400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.7456 | top1:  38.1200 | top5:  88.1500\n",
      "\n",
      "Epoch: [3 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.8729 | Loss_x: 0.8709 | Loss_u: 0.0108 | W: 0.1831\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.1167 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.5109 | top1:  48.8400 | top5:  90.7200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.5019 | top1:  48.9000 | top5:  90.8400\n",
      "\n",
      "Epoch: [4 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.8618 | Loss_x: 0.8590 | Loss_u: 0.0108 | W: 0.2563\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0717 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.4139 | top1:  53.3800 | top5:  91.2400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.4047 | top1:  52.7100 | top5:  91.6000\n",
      "\n",
      "Epoch: [5 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.8195 | Loss_x: 0.8160 | Loss_u: 0.0105 | W: 0.3296\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0625 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.3702 | top1:  54.8800 | top5:  91.5800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.3639 | top1:  54.4000 | top5:  91.6100\n",
      "\n",
      "Epoch: [6 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.8166 | Loss_x: 0.8123 | Loss_u: 0.0105 | W: 0.4028\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0687 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.3383 | top1:  55.7200 | top5:  92.1000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.3380 | top1:  55.8800 | top5:  91.7500\n",
      "\n",
      "Epoch: [7 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.8111 | Loss_x: 0.8060 | Loss_u: 0.0107 | W: 0.4760\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0738 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.3153 | top1:  56.8000 | top5:  92.0600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.3212 | top1:  56.6800 | top5:  91.8800\n",
      "\n",
      "Epoch: [8 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7743 | Loss_x: 0.7684 | Loss_u: 0.0108 | W: 0.5493\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0757 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2793 | top1:  58.2000 | top5:  92.5200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2867 | top1:  58.0700 | top5:  92.3600\n",
      "\n",
      "Epoch: [9 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7840 | Loss_x: 0.7771 | Loss_u: 0.0110 | W: 0.6225\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0774 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2637 | top1:  58.9800 | top5:  92.9800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2750 | top1:  58.8400 | top5:  92.6000\n",
      "\n",
      "Epoch: [10 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7950 | Loss_x: 0.7871 | Loss_u: 0.0113 | W: 0.6958\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0765 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2576 | top1:  59.8200 | top5:  92.8600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2691 | top1:  59.2700 | top5:  92.6000\n",
      "\n",
      "Epoch: [11 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7583 | Loss_x: 0.7496 | Loss_u: 0.0113 | W: 0.7690\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0695 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2332 | top1:  60.5800 | top5:  93.3600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2486 | top1:  59.8100 | top5:  92.8200\n",
      "\n",
      "Epoch: [12 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7440 | Loss_x: 0.7345 | Loss_u: 0.0113 | W: 0.8422\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0629 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2129 | top1:  61.0600 | top5:  93.3400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2284 | top1:  61.2100 | top5:  93.1300\n",
      "\n",
      "Epoch: [13 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7512 | Loss_x: 0.7406 | Loss_u: 0.0116 | W: 0.9155\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0569 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2136 | top1:  61.7000 | top5:  93.2600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2336 | top1:  61.1800 | top5:  92.9900\n",
      "\n",
      "Epoch: [14 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7483 | Loss_x: 0.7367 | Loss_u: 0.0117 | W: 0.9887\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0527 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2060 | top1:  62.2400 | top5:  92.9400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2293 | top1:  61.7300 | top5:  93.0400\n",
      "\n",
      "Epoch: [15 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7386 | Loss_x: 0.7261 | Loss_u: 0.0117 | W: 1.0620\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0488 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1974 | top1:  62.5600 | top5:  92.6400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.2164 | top1:  62.3800 | top5:  93.1300\n",
      "\n",
      "Epoch: [16 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7374 | Loss_x: 0.7240 | Loss_u: 0.0118 | W: 1.1352\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0511 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1662 | top1:  63.2600 | top5:  93.2800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1890 | top1:  63.0500 | top5:  93.6500\n",
      "\n",
      "Epoch: [17 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7522 | Loss_x: 0.7379 | Loss_u: 0.0118 | W: 1.2085\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0521 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1728 | top1:  63.5400 | top5:  93.3200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1948 | top1:  63.1500 | top5:  93.4600\n",
      "\n",
      "Epoch: [18 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7391 | Loss_x: 0.7239 | Loss_u: 0.0119 | W: 1.2817\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0490 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1477 | top1:  63.9400 | top5:  93.3800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1679 | top1:  64.1600 | top5:  93.7800\n",
      "\n",
      "Epoch: [19 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7230 | Loss_x: 0.7072 | Loss_u: 0.0117 | W: 1.3549\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0487 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1466 | top1:  64.0400 | top5:  94.0800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1674 | top1:  64.3000 | top5:  93.8600\n",
      "\n",
      "Epoch: [20 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7255 | Loss_x: 0.7087 | Loss_u: 0.0118 | W: 1.4282\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0463 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1673 | top1:  63.9400 | top5:  93.4600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1901 | top1:  63.8800 | top5:  93.4700\n",
      "\n",
      "Epoch: [21 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.068s | Total: 0:01:09 | ETA: 0:00:01 | Loss: 0.7106 | Loss_x: 0.6931 | Loss_u: 0.0117 | W: 1.5014\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0436 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1567 | top1:  64.5800 | top5:  93.7000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1792 | top1:  64.3200 | top5:  93.8700\n",
      "\n",
      "Epoch: [22 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7181 | Loss_x: 0.6996 | Loss_u: 0.0117 | W: 1.5747\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0441 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1484 | top1:  64.7800 | top5:  94.1000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1751 | top1:  64.5500 | top5:  93.9400\n",
      "\n",
      "Epoch: [23 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7309 | Loss_x: 0.7113 | Loss_u: 0.0119 | W: 1.6479\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0446 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1677 | top1:  64.4000 | top5:  93.7800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1995 | top1:  64.1700 | top5:  93.5200\n",
      "\n",
      "Epoch: [24 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7070 | Loss_x: 0.6869 | Loss_u: 0.0117 | W: 1.7212\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0457 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1477 | top1:  64.8200 | top5:  93.9600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1856 | top1:  64.5600 | top5:  93.8000\n",
      "\n",
      "Epoch: [25 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7228 | Loss_x: 0.7019 | Loss_u: 0.0117 | W: 1.7944\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0489 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1288 | top1:  65.2800 | top5:  94.2600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1588 | top1:  65.1800 | top5:  94.2800\n",
      "\n",
      "Epoch: [26 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.069s | Total: 0:01:10 | ETA: 0:00:01 | Loss: 0.7064 | Loss_x: 0.6848 | Loss_u: 0.0116 | W: 1.8676\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0486 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1311 | top1:  65.9600 | top5:  94.1000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1582 | top1:  65.4100 | top5:  94.1300\n",
      "\n",
      "Epoch: [27 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7120 | Loss_x: 0.6899 | Loss_u: 0.0114 | W: 1.9409\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0511 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1381 | top1:  65.9800 | top5:  93.7000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1669 | top1:  65.6700 | top5:  93.7800\n",
      "\n",
      "Epoch: [28 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6992 | Loss_x: 0.6762 | Loss_u: 0.0114 | W: 2.0141\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0518 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1147 | top1:  66.2400 | top5:  94.0800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1417 | top1:  66.2800 | top5:  94.2500\n",
      "\n",
      "Epoch: [29 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7056 | Loss_x: 0.6817 | Loss_u: 0.0114 | W: 2.0874\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0494 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1160 | top1:  66.0600 | top5:  94.4600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1434 | top1:  66.2800 | top5:  94.4100\n",
      "\n",
      "Epoch: [30 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7014 | Loss_x: 0.6768 | Loss_u: 0.0114 | W: 2.1606\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0498 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0978 | top1:  67.1000 | top5:  94.5200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1273 | top1:  66.6100 | top5:  94.6300\n",
      "\n",
      "Epoch: [31 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.067s | Total: 0:01:08 | ETA: 0:00:01 | Loss: 0.7091 | Loss_x: 0.6835 | Loss_u: 0.0114 | W: 2.2339\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0497 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0689 | top1:  67.5800 | top5:  94.7600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0971 | top1:  67.1600 | top5:  94.7800\n",
      "\n",
      "Epoch: [32 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.067s | Total: 0:01:08 | ETA: 0:00:01 | Loss: 0.7102 | Loss_x: 0.6835 | Loss_u: 0.0116 | W: 2.3071\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0521 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0726 | top1:  67.9800 | top5:  94.9000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.1017 | top1:  67.1900 | top5:  94.8100\n",
      "\n",
      "Epoch: [33 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6931 | Loss_x: 0.6663 | Loss_u: 0.0113 | W: 2.3803\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0523 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0509 | top1:  68.3600 | top5:  94.8800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0772 | top1:  67.7800 | top5:  94.9300\n",
      "\n",
      "Epoch: [34 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7104 | Loss_x: 0.6825 | Loss_u: 0.0114 | W: 2.4536\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0521 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0361 | top1:  69.1600 | top5:  94.9600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0607 | top1:  68.1300 | top5:  94.9400\n",
      "\n",
      "Epoch: [35 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7168 | Loss_x: 0.6875 | Loss_u: 0.0116 | W: 2.5268\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0537 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0481 | top1:  69.1200 | top5:  95.0000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0754 | top1:  68.0900 | top5:  94.8500\n",
      "\n",
      "Epoch: [36 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7026 | Loss_x: 0.6728 | Loss_u: 0.0115 | W: 2.6001\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0518 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0126 | top1:  69.9000 | top5:  95.5000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0353 | top1:  69.0700 | top5:  95.3500\n",
      "\n",
      "Epoch: [37 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7274 | Loss_x: 0.6962 | Loss_u: 0.0117 | W: 2.6733\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0529 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9956 | top1:  69.9400 | top5:  95.4200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0150 | top1:  69.6900 | top5:  95.4000\n",
      "\n",
      "Epoch: [38 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7061 | Loss_x: 0.6749 | Loss_u: 0.0114 | W: 2.7465\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0535 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9972 | top1:  70.1200 | top5:  95.6000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0183 | top1:  69.7300 | top5:  95.5900\n",
      "\n",
      "Epoch: [39 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6948 | Loss_x: 0.6636 | Loss_u: 0.0111 | W: 2.8198\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0560 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9794 | top1:  70.9600 | top5:  95.7000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0009 | top1:  70.4300 | top5:  95.4700\n",
      "\n",
      "Epoch: [40 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6946 | Loss_x: 0.6626 | Loss_u: 0.0111 | W: 2.8930\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0556 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9835 | top1:  71.1800 | top5:  95.4200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 1.0076 | top1:  70.5800 | top5:  95.3800\n",
      "\n",
      "Epoch: [41 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7069 | Loss_x: 0.6737 | Loss_u: 0.0112 | W: 2.9663\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0555 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9768 | top1:  71.5600 | top5:  95.3800\n",
      "Test Stats  |################################| (157/157) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9973 | top1:  70.9100 | top5:  95.3000\n",
      "\n",
      "Epoch: [42 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.063s | Total: 0:01:04 | ETA: 0:00:01 | Loss: 0.7211 | Loss_x: 0.6862 | Loss_u: 0.0115 | W: 3.0395\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0559 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9619 | top1:  71.8600 | top5:  95.7200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9786 | top1:  71.0600 | top5:  95.6800\n",
      "\n",
      "Epoch: [43 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.067s | Total: 0:01:08 | ETA: 0:00:01 | Loss: 0.7068 | Loss_x: 0.6721 | Loss_u: 0.0111 | W: 3.1128\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0604 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9715 | top1:  71.5600 | top5:  95.7200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9918 | top1:  70.7800 | top5:  95.7400\n",
      "\n",
      "Epoch: [44 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7124 | Loss_x: 0.6766 | Loss_u: 0.0112 | W: 3.1860\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0567 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9653 | top1:  71.4200 | top5:  95.4600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9918 | top1:  71.1400 | top5:  95.5400\n",
      "\n",
      "Epoch: [45 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6855 | Loss_x: 0.6500 | Loss_u: 0.0109 | W: 3.2592\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0575 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9464 | top1:  72.2200 | top5:  95.8200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9699 | top1:  71.7700 | top5:  95.6400\n",
      "\n",
      "Epoch: [46 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6959 | Loss_x: 0.6593 | Loss_u: 0.0110 | W: 3.3325\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0560 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9637 | top1:  71.9000 | top5:  95.5600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9878 | top1:  71.6700 | top5:  95.3300\n",
      "\n",
      "Epoch: [47 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6965 | Loss_x: 0.6589 | Loss_u: 0.0110 | W: 3.4057\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0576 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9427 | top1:  72.5800 | top5:  95.9400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9663 | top1:  72.2700 | top5:  95.4900\n",
      "\n",
      "Epoch: [48 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.6958 | Loss_x: 0.6576 | Loss_u: 0.0110 | W: 3.4790\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0570 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9446 | top1:  73.3400 | top5:  95.8000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9656 | top1:  72.5400 | top5:  95.5500\n",
      "\n",
      "Epoch: [49 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6944 | Loss_x: 0.6558 | Loss_u: 0.0109 | W: 3.5522\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0553 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9338 | top1:  73.3400 | top5:  95.8600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9552 | top1:  72.6800 | top5:  95.8100\n",
      "\n",
      "Epoch: [50 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6970 | Loss_x: 0.6579 | Loss_u: 0.0108 | W: 3.6255\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0593 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9380 | top1:  73.4400 | top5:  96.0600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9654 | top1:  72.2400 | top5:  95.9200\n",
      "\n",
      "Epoch: [51 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.023s | Batch: 0.079s | Total: 0:01:20 | ETA: 0:00:01 | Loss: 0.6961 | Loss_x: 0.6563 | Loss_u: 0.0108 | W: 3.6987\n",
      "Train Stats |################################| (3/3) Data: 0.011s | Batch: 0.016s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0575 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.006s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9317 | top1:  73.5600 | top5:  95.9200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.005s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9530 | top1:  73.0500 | top5:  95.7100\n",
      "\n",
      "Epoch: [52 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.021s | Batch: 0.075s | Total: 0:01:16 | ETA: 0:00:01 | Loss: 0.7168 | Loss_x: 0.6749 | Loss_u: 0.0111 | W: 3.7719\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0607 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9116 | top1:  73.7800 | top5:  96.3200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9343 | top1:  73.1800 | top5:  95.9600\n",
      "\n",
      "Epoch: [53 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6952 | Loss_x: 0.6539 | Loss_u: 0.0107 | W: 3.8452\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0593 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9123 | top1:  73.8200 | top5:  96.1400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9332 | top1:  73.2700 | top5:  95.8400\n",
      "\n",
      "Epoch: [54 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7035 | Loss_x: 0.6612 | Loss_u: 0.0108 | W: 3.9184\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0595 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9193 | top1:  73.7200 | top5:  95.9200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9405 | top1:  73.2100 | top5:  95.7000\n",
      "\n",
      "Epoch: [55 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7169 | Loss_x: 0.6731 | Loss_u: 0.0110 | W: 3.9917\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0619 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9111 | top1:  74.0800 | top5:  96.0600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9309 | top1:  73.3300 | top5:  95.8600\n",
      "\n",
      "Epoch: [56 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6994 | Loss_x: 0.6558 | Loss_u: 0.0107 | W: 4.0649\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0633 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9101 | top1:  73.8200 | top5:  96.1800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9326 | top1:  73.4600 | top5:  95.8200\n",
      "\n",
      "Epoch: [57 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7150 | Loss_x: 0.6698 | Loss_u: 0.0109 | W: 4.1381\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0595 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9168 | top1:  73.7200 | top5:  96.0600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9390 | top1:  73.3200 | top5:  95.8200\n",
      "\n",
      "Epoch: [58 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7092 | Loss_x: 0.6638 | Loss_u: 0.0108 | W: 4.2114\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0613 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9162 | top1:  74.0400 | top5:  96.0800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9375 | top1:  73.5600 | top5:  95.6800\n",
      "\n",
      "Epoch: [59 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6924 | Loss_x: 0.6476 | Loss_u: 0.0104 | W: 4.2846\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0603 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9192 | top1:  74.0600 | top5:  96.1600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9432 | top1:  73.6000 | top5:  95.9100\n",
      "\n",
      "Epoch: [60 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7135 | Loss_x: 0.6665 | Loss_u: 0.0108 | W: 4.3579\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0621 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9107 | top1:  73.8400 | top5:  96.4000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9320 | top1:  73.7200 | top5:  95.9900\n",
      "\n",
      "Epoch: [61 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7055 | Loss_x: 0.6580 | Loss_u: 0.0107 | W: 4.4311\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0579 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9145 | top1:  74.3000 | top5:  96.2400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9350 | top1:  73.9200 | top5:  95.9300\n",
      "\n",
      "Epoch: [62 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7124 | Loss_x: 0.6648 | Loss_u: 0.0106 | W: 4.5044\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0613 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9121 | top1:  74.2200 | top5:  96.2200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9361 | top1:  73.9900 | top5:  95.7900\n",
      "\n",
      "Epoch: [63 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7001 | Loss_x: 0.6519 | Loss_u: 0.0105 | W: 4.5776\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0579 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9006 | top1:  74.7000 | top5:  96.4600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9292 | top1:  73.9600 | top5:  95.7600\n",
      "\n",
      "Epoch: [64 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.067s | Total: 0:01:08 | ETA: 0:00:01 | Loss: 0.6942 | Loss_x: 0.6461 | Loss_u: 0.0103 | W: 4.6508\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0578 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9124 | top1:  74.3400 | top5:  96.3200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9406 | top1:  73.7700 | top5:  95.8300\n",
      "\n",
      "Epoch: [65 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6936 | Loss_x: 0.6449 | Loss_u: 0.0103 | W: 4.7241\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0595 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9020 | top1:  74.2200 | top5:  96.4400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9349 | top1:  73.9200 | top5:  95.8200\n",
      "\n",
      "Epoch: [66 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6949 | Loss_x: 0.6457 | Loss_u: 0.0103 | W: 4.7973\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0575 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8914 | top1:  74.6200 | top5:  96.5600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9211 | top1:  74.2000 | top5:  95.9700\n",
      "\n",
      "Epoch: [67 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7165 | Loss_x: 0.6647 | Loss_u: 0.0106 | W: 4.8706\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0610 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9132 | top1:  74.6000 | top5:  96.1000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9412 | top1:  74.1200 | top5:  95.6900\n",
      "\n",
      "Epoch: [68 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.070s | Total: 0:01:11 | ETA: 0:00:01 | Loss: 0.7147 | Loss_x: 0.6633 | Loss_u: 0.0104 | W: 4.9438\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0632 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9160 | top1:  74.6200 | top5:  96.1000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9387 | top1:  73.9700 | top5:  95.8100\n",
      "\n",
      "Epoch: [69 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.6970 | Loss_x: 0.6458 | Loss_u: 0.0102 | W: 5.0171\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0622 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9056 | top1:  74.5800 | top5:  96.2800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9196 | top1:  74.5200 | top5:  95.8800\n",
      "\n",
      "Epoch: [70 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.6931 | Loss_x: 0.6412 | Loss_u: 0.0102 | W: 5.0903\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0617 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8969 | top1:  74.8400 | top5:  96.2000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9177 | top1:  74.7100 | top5:  95.9000\n",
      "\n",
      "Epoch: [71 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7178 | Loss_x: 0.6638 | Loss_u: 0.0104 | W: 5.1635\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0654 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9111 | top1:  75.0200 | top5:  96.2400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9325 | top1:  74.3800 | top5:  95.9600\n",
      "\n",
      "Epoch: [72 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7067 | Loss_x: 0.6525 | Loss_u: 0.0103 | W: 5.2368\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0635 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9057 | top1:  74.8400 | top5:  96.3600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9274 | top1:  74.6400 | top5:  95.9200\n",
      "\n",
      "Epoch: [73 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.068s | Total: 0:01:09 | ETA: 0:00:01 | Loss: 0.7064 | Loss_x: 0.6523 | Loss_u: 0.0102 | W: 5.3100\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0647 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9149 | top1:  74.5000 | top5:  96.4600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9392 | top1:  74.1900 | top5:  95.9600\n",
      "\n",
      "Epoch: [74 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.069s | Total: 0:01:10 | ETA: 0:00:01 | Loss: 0.7077 | Loss_x: 0.6530 | Loss_u: 0.0102 | W: 5.3833\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0667 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9141 | top1:  74.7200 | top5:  96.3800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9335 | top1:  74.3900 | top5:  95.9000\n",
      "\n",
      "Epoch: [75 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7204 | Loss_x: 0.6639 | Loss_u: 0.0104 | W: 5.4565\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0656 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9214 | top1:  74.6400 | top5:  96.1400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9443 | top1:  74.2100 | top5:  95.7200\n",
      "\n",
      "Epoch: [76 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6978 | Loss_x: 0.6426 | Loss_u: 0.0100 | W: 5.5297\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0659 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9101 | top1:  74.5600 | top5:  96.3800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9321 | top1:  74.5200 | top5:  95.8800\n",
      "\n",
      "Epoch: [77 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6946 | Loss_x: 0.6388 | Loss_u: 0.0100 | W: 5.6030\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0636 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9010 | top1:  74.8600 | top5:  96.6200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9199 | top1:  74.7600 | top5:  96.0200\n",
      "\n",
      "Epoch: [78 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7057 | Loss_x: 0.6479 | Loss_u: 0.0102 | W: 5.6762\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0627 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9120 | top1:  74.6600 | top5:  96.5400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9246 | top1:  74.7300 | top5:  96.0300\n",
      "\n",
      "Epoch: [79 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7068 | Loss_x: 0.6494 | Loss_u: 0.0100 | W: 5.7495\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0621 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9079 | top1:  74.6800 | top5:  96.5400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9242 | top1:  74.7100 | top5:  96.1400\n",
      "\n",
      "Epoch: [80 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7031 | Loss_x: 0.6447 | Loss_u: 0.0100 | W: 5.8227\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0656 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9129 | top1:  75.0600 | top5:  96.1800\n",
      "Test Stats  |################################| (157/157) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9313 | top1:  74.4100 | top5:  95.9500\n",
      "\n",
      "Epoch: [81 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7039 | Loss_x: 0.6455 | Loss_u: 0.0099 | W: 5.8960\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0655 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9043 | top1:  75.5600 | top5:  96.2800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9249 | top1:  74.7800 | top5:  95.9800\n",
      "\n",
      "Epoch: [82 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7210 | Loss_x: 0.6601 | Loss_u: 0.0102 | W: 5.9692\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0656 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8925 | top1:  75.3000 | top5:  96.1200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9130 | top1:  75.0500 | top5:  95.9600\n",
      "\n",
      "Epoch: [83 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6878 | Loss_x: 0.6290 | Loss_u: 0.0097 | W: 6.0424\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0614 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8849 | top1:  75.7800 | top5:  96.4600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9054 | top1:  75.3100 | top5:  96.0500\n",
      "\n",
      "Epoch: [84 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7124 | Loss_x: 0.6508 | Loss_u: 0.0101 | W: 6.1157\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0629 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8878 | top1:  76.0800 | top5:  96.6000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9127 | top1:  75.2800 | top5:  95.9300\n",
      "\n",
      "Epoch: [85 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7116 | Loss_x: 0.6495 | Loss_u: 0.0100 | W: 6.1889\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0652 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8917 | top1:  75.6000 | top5:  96.5000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9202 | top1:  74.9800 | top5:  95.8600\n",
      "\n",
      "Epoch: [86 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6852 | Loss_x: 0.6247 | Loss_u: 0.0097 | W: 6.2622\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0605 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8858 | top1:  75.9000 | top5:  96.5000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9196 | top1:  75.1200 | top5:  96.0700\n",
      "\n",
      "Epoch: [87 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.6967 | Loss_x: 0.6351 | Loss_u: 0.0097 | W: 6.3354\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0617 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8965 | top1:  75.7000 | top5:  96.4400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9260 | top1:  75.2800 | top5:  95.9100\n",
      "\n",
      "Epoch: [88 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7053 | Loss_x: 0.6423 | Loss_u: 0.0098 | W: 6.4087\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0653 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9166 | top1:  75.1600 | top5:  96.1600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9428 | top1:  74.9400 | top5:  95.8400\n",
      "\n",
      "Epoch: [89 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6982 | Loss_x: 0.6355 | Loss_u: 0.0097 | W: 6.4819\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0634 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8973 | top1:  75.7400 | top5:  96.4000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9167 | top1:  75.3000 | top5:  95.9900\n",
      "\n",
      "Epoch: [90 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.6973 | Loss_x: 0.6334 | Loss_u: 0.0097 | W: 6.5551\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0610 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8986 | top1:  75.3800 | top5:  96.3600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9197 | top1:  75.2400 | top5:  96.0200\n",
      "\n",
      "Epoch: [91 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7142 | Loss_x: 0.6489 | Loss_u: 0.0099 | W: 6.6284\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0627 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8800 | top1:  76.4200 | top5:  96.2600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9061 | top1:  75.9300 | top5:  95.8400\n",
      "\n",
      "Epoch: [92 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7100 | Loss_x: 0.6437 | Loss_u: 0.0099 | W: 6.7016\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0651 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8921 | top1:  75.6800 | top5:  96.0000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9213 | top1:  75.6400 | top5:  95.9200\n",
      "\n",
      "Epoch: [93 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7011 | Loss_x: 0.6359 | Loss_u: 0.0096 | W: 6.7749\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0643 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8892 | top1:  75.9000 | top5:  96.2600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9205 | top1:  75.5300 | top5:  95.9300\n",
      "\n",
      "Epoch: [94 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7168 | Loss_x: 0.6503 | Loss_u: 0.0097 | W: 6.8481\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0701 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8697 | top1:  76.0400 | top5:  96.5000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8945 | top1:  75.8400 | top5:  96.1400\n",
      "\n",
      "Epoch: [95 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7287 | Loss_x: 0.6600 | Loss_u: 0.0099 | W: 6.9214\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0652 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8648 | top1:  76.3200 | top5:  96.5000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8907 | top1:  76.3000 | top5:  96.0200\n",
      "\n",
      "Epoch: [96 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7142 | Loss_x: 0.6459 | Loss_u: 0.0098 | W: 6.9946\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0639 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8721 | top1:  76.3600 | top5:  96.5400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8964 | top1:  75.9100 | top5:  96.1200\n",
      "\n",
      "Epoch: [97 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7252 | Loss_x: 0.6553 | Loss_u: 0.0099 | W: 7.0678\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0691 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8844 | top1:  76.2200 | top5:  96.2800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9052 | top1:  75.8000 | top5:  96.0300\n",
      "\n",
      "Epoch: [98 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7169 | Loss_x: 0.6471 | Loss_u: 0.0098 | W: 7.1411\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0704 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8864 | top1:  75.8400 | top5:  96.1800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9131 | top1:  75.5100 | top5:  96.0200\n",
      "\n",
      "Epoch: [99 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7121 | Loss_x: 0.6421 | Loss_u: 0.0097 | W: 7.2143\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0672 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8753 | top1:  76.4000 | top5:  96.4800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8968 | top1:  75.9500 | top5:  96.1800\n",
      "\n",
      "Epoch: [100 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7132 | Loss_x: 0.6425 | Loss_u: 0.0097 | W: 7.2876\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0652 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8785 | top1:  76.3800 | top5:  96.5000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.9006 | top1:  76.1400 | top5:  96.1600\n",
      "\n",
      "Epoch: [101 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7099 | Loss_x: 0.6397 | Loss_u: 0.0095 | W: 7.3608\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0659 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8694 | top1:  76.9600 | top5:  96.5000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8913 | top1:  76.3100 | top5:  96.2200\n",
      "\n",
      "Epoch: [102 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7405 | Loss_x: 0.6667 | Loss_u: 0.0099 | W: 7.4340\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0743 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8485 | top1:  77.2200 | top5:  96.7800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8709 | top1:  76.6900 | top5:  96.2400\n",
      "\n",
      "Epoch: [103 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7072 | Loss_x: 0.6364 | Loss_u: 0.0094 | W: 7.5073\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0726 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8467 | top1:  77.0800 | top5:  96.8400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8654 | top1:  76.8000 | top5:  96.2200\n",
      "\n",
      "Epoch: [104 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7147 | Loss_x: 0.6423 | Loss_u: 0.0096 | W: 7.5805\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0728 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8668 | top1:  76.6600 | top5:  96.7200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8917 | top1:  76.4300 | top5:  96.1800\n",
      "\n",
      "Epoch: [105 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7223 | Loss_x: 0.6477 | Loss_u: 0.0098 | W: 7.6538\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0667 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8594 | top1:  76.6200 | top5:  96.6200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8810 | top1:  76.4500 | top5:  96.0700\n",
      "\n",
      "Epoch: [106 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7224 | Loss_x: 0.6481 | Loss_u: 0.0096 | W: 7.7270\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0645 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8526 | top1:  76.9800 | top5:  96.8800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8801 | top1:  76.7000 | top5:  96.2400\n",
      "\n",
      "Epoch: [107 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7260 | Loss_x: 0.6509 | Loss_u: 0.0096 | W: 7.8003\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0703 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8358 | top1:  77.5600 | top5:  96.8400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8652 | top1:  76.9700 | top5:  96.2700\n",
      "\n",
      "Epoch: [108 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7130 | Loss_x: 0.6392 | Loss_u: 0.0094 | W: 7.8735\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0683 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8216 | top1:  77.8600 | top5:  96.8600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8538 | top1:  77.1200 | top5:  96.4100\n",
      "\n",
      "Epoch: [109 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7287 | Loss_x: 0.6529 | Loss_u: 0.0095 | W: 7.9467\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0677 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8279 | top1:  78.0600 | top5:  96.8800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8600 | top1:  76.9900 | top5:  96.3500\n",
      "\n",
      "Epoch: [110 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7077 | Loss_x: 0.6335 | Loss_u: 0.0093 | W: 8.0200\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0675 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8353 | top1:  77.5800 | top5:  96.9800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8641 | top1:  76.7700 | top5:  96.4600\n",
      "\n",
      "Epoch: [111 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7170 | Loss_x: 0.6410 | Loss_u: 0.0094 | W: 8.0932\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0734 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8343 | top1:  77.5000 | top5:  96.9400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8650 | top1:  76.8700 | top5:  96.3900\n",
      "\n",
      "Epoch: [112 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7028 | Loss_x: 0.6266 | Loss_u: 0.0093 | W: 8.1665\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0720 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8329 | top1:  77.4000 | top5:  96.8600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8582 | top1:  77.1000 | top5:  96.2500\n",
      "\n",
      "Epoch: [113 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7207 | Loss_x: 0.6427 | Loss_u: 0.0095 | W: 8.2397\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0685 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8332 | top1:  77.3000 | top5:  96.6600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8612 | top1:  76.9500 | top5:  96.3500\n",
      "\n",
      "Epoch: [114 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7280 | Loss_x: 0.6479 | Loss_u: 0.0096 | W: 8.3130\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0672 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8284 | top1:  77.4800 | top5:  96.7800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8566 | top1:  77.2500 | top5:  96.1500\n",
      "\n",
      "Epoch: [115 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7061 | Loss_x: 0.6289 | Loss_u: 0.0092 | W: 8.3862\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0714 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8287 | top1:  77.5200 | top5:  96.7800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8534 | top1:  77.1200 | top5:  96.4100\n",
      "\n",
      "Epoch: [116 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7382 | Loss_x: 0.6564 | Loss_u: 0.0097 | W: 8.4594\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0711 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8330 | top1:  77.2200 | top5:  96.8600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8552 | top1:  77.3500 | top5:  96.2800\n",
      "\n",
      "Epoch: [117 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7060 | Loss_x: 0.6281 | Loss_u: 0.0091 | W: 8.5327\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0686 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8395 | top1:  77.3800 | top5:  96.9400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8620 | top1:  77.0900 | top5:  96.3700\n",
      "\n",
      "Epoch: [118 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7291 | Loss_x: 0.6479 | Loss_u: 0.0094 | W: 8.6059\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0703 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8450 | top1:  77.2400 | top5:  96.8600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8710 | top1:  76.8800 | top5:  96.2100\n",
      "\n",
      "Epoch: [119 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7201 | Loss_x: 0.6405 | Loss_u: 0.0092 | W: 8.6792\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0686 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8422 | top1:  77.4600 | top5:  96.7800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8643 | top1:  76.7400 | top5:  96.4600\n",
      "\n",
      "Epoch: [120 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7382 | Loss_x: 0.6554 | Loss_u: 0.0095 | W: 8.7524\n",
      "Train Stats |################################| (3/3) Data: 0.008s | Batch: 0.012s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0721 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.005s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8333 | top1:  77.3800 | top5:  96.8600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.005s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8591 | top1:  76.9100 | top5:  96.4000\n",
      "\n",
      "Epoch: [121 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7166 | Loss_x: 0.6347 | Loss_u: 0.0093 | W: 8.8256\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0704 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8134 | top1:  78.3000 | top5:  96.9400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8395 | top1:  77.4900 | top5:  96.5100\n",
      "\n",
      "Epoch: [122 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7288 | Loss_x: 0.6462 | Loss_u: 0.0093 | W: 8.8989\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0709 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8262 | top1:  78.0000 | top5:  97.0000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8478 | top1:  77.4600 | top5:  96.5100\n",
      "\n",
      "Epoch: [123 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7171 | Loss_x: 0.6343 | Loss_u: 0.0092 | W: 8.9721\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0703 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8282 | top1:  77.9400 | top5:  96.7600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8489 | top1:  77.2400 | top5:  96.3600\n",
      "\n",
      "Epoch: [124 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7284 | Loss_x: 0.6446 | Loss_u: 0.0093 | W: 9.0454\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0706 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8230 | top1:  78.0800 | top5:  96.7600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8535 | top1:  77.2300 | top5:  96.3800\n",
      "\n",
      "Epoch: [125 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7180 | Loss_x: 0.6346 | Loss_u: 0.0091 | W: 9.1186\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0678 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8107 | top1:  78.2600 | top5:  96.7800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8400 | top1:  77.4600 | top5:  96.2300\n",
      "\n",
      "Epoch: [126 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.068s | Total: 0:01:09 | ETA: 0:00:01 | Loss: 0.7203 | Loss_x: 0.6366 | Loss_u: 0.0091 | W: 9.1919\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0701 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8135 | top1:  78.0400 | top5:  96.7800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8496 | top1:  77.5000 | top5:  96.4000\n",
      "\n",
      "Epoch: [127 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7380 | Loss_x: 0.6502 | Loss_u: 0.0095 | W: 9.2651\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0720 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8151 | top1:  77.8800 | top5:  96.9000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8472 | top1:  77.6500 | top5:  96.4900\n",
      "\n",
      "Epoch: [128 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7208 | Loss_x: 0.6357 | Loss_u: 0.0091 | W: 9.3383\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0706 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8262 | top1:  78.0200 | top5:  96.9200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8556 | top1:  77.4800 | top5:  96.4600\n",
      "\n",
      "Epoch: [129 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7163 | Loss_x: 0.6305 | Loss_u: 0.0091 | W: 9.4116\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0676 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8223 | top1:  78.0600 | top5:  96.5800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8557 | top1:  77.7200 | top5:  96.2400\n",
      "\n",
      "Epoch: [130 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7316 | Loss_x: 0.6433 | Loss_u: 0.0093 | W: 9.4848\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0699 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8051 | top1:  78.4400 | top5:  96.8800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8428 | top1:  77.9500 | top5:  96.3700\n",
      "\n",
      "Epoch: [131 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7325 | Loss_x: 0.6444 | Loss_u: 0.0092 | W: 9.5581\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0684 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7960 | top1:  78.9200 | top5:  97.0600\n",
      "Test Stats  |################################| (157/157) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8347 | top1:  77.9900 | top5:  96.5100\n",
      "\n",
      "Epoch: [132 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.063s | Total: 0:01:04 | ETA: 0:00:01 | Loss: 0.7280 | Loss_x: 0.6396 | Loss_u: 0.0092 | W: 9.6313\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0679 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7896 | top1:  78.8400 | top5:  97.2000\n",
      "Test Stats  |################################| (157/157) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8237 | top1:  78.0400 | top5:  96.6900\n",
      "\n",
      "Epoch: [133 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.016s | Batch: 0.062s | Total: 0:01:03 | ETA: 0:00:01 | Loss: 0.7182 | Loss_x: 0.6306 | Loss_u: 0.0090 | W: 9.7046\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0680 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7886 | top1:  79.1400 | top5:  97.2200\n",
      "Test Stats  |################################| (157/157) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8222 | top1:  78.4400 | top5:  96.7600\n",
      "\n",
      "Epoch: [134 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.016s | Batch: 0.062s | Total: 0:01:03 | ETA: 0:00:01 | Loss: 0.7310 | Loss_x: 0.6417 | Loss_u: 0.0091 | W: 9.7778\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0721 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7739 | top1:  79.4200 | top5:  97.2600\n",
      "Test Stats  |################################| (157/157) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8060 | top1:  78.9200 | top5:  96.6800\n",
      "\n",
      "Epoch: [135 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7362 | Loss_x: 0.6462 | Loss_u: 0.0091 | W: 9.8510\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0725 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7835 | top1:  79.0000 | top5:  97.1400\n",
      "Test Stats  |################################| (157/157) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8208 | top1:  78.1300 | top5:  96.6200\n",
      "\n",
      "Epoch: [136 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.063s | Total: 0:01:04 | ETA: 0:00:01 | Loss: 0.7263 | Loss_x: 0.6373 | Loss_u: 0.0090 | W: 9.9243\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0710 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7808 | top1:  79.4800 | top5:  97.1800\n",
      "Test Stats  |################################| (157/157) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8215 | top1:  78.2900 | top5:  96.5700\n",
      "\n",
      "Epoch: [137 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.063s | Total: 0:01:04 | ETA: 0:00:01 | Loss: 0.7261 | Loss_x: 0.6356 | Loss_u: 0.0090 | W: 9.9975\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0687 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7881 | top1:  79.0800 | top5:  97.0200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8256 | top1:  78.3000 | top5:  96.4300\n",
      "\n",
      "Epoch: [138 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.063s | Total: 0:01:04 | ETA: 0:00:01 | Loss: 0.7359 | Loss_x: 0.6436 | Loss_u: 0.0092 | W: 10.0708\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0677 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7889 | top1:  79.0000 | top5:  96.8800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8259 | top1:  78.3700 | top5:  96.4400\n",
      "\n",
      "Epoch: [139 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.063s | Total: 0:01:04 | ETA: 0:00:01 | Loss: 0.7079 | Loss_x: 0.6196 | Loss_u: 0.0087 | W: 10.1440\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0689 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7709 | top1:  79.0800 | top5:  97.1800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8069 | top1:  78.5500 | top5:  96.5800\n",
      "\n",
      "Epoch: [140 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.063s | Total: 0:01:04 | ETA: 0:00:01 | Loss: 0.7335 | Loss_x: 0.6417 | Loss_u: 0.0090 | W: 10.2172\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0732 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7809 | top1:  79.2400 | top5:  96.9000\n",
      "Test Stats  |################################| (157/157) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8130 | top1:  78.5700 | top5:  96.4500\n",
      "\n",
      "Epoch: [141 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.063s | Total: 0:01:04 | ETA: 0:00:01 | Loss: 0.7264 | Loss_x: 0.6337 | Loss_u: 0.0090 | W: 10.2905\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0716 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7860 | top1:  79.3200 | top5:  97.1000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8205 | top1:  78.4200 | top5:  96.6200\n",
      "\n",
      "Epoch: [142 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.063s | Total: 0:01:04 | ETA: 0:00:01 | Loss: 0.7270 | Loss_x: 0.6336 | Loss_u: 0.0090 | W: 10.3637\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0696 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7905 | top1:  79.0600 | top5:  96.9800\n",
      "Test Stats  |################################| (157/157) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8279 | top1:  78.4200 | top5:  96.6000\n",
      "\n",
      "Epoch: [143 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.063s | Total: 0:01:04 | ETA: 0:00:01 | Loss: 0.7349 | Loss_x: 0.6398 | Loss_u: 0.0091 | W: 10.4370\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0692 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7658 | top1:  79.5600 | top5:  97.1200\n",
      "Test Stats  |################################| (157/157) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8037 | top1:  78.6000 | top5:  96.6800\n",
      "\n",
      "Epoch: [144 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.063s | Total: 0:01:04 | ETA: 0:00:01 | Loss: 0.7348 | Loss_x: 0.6397 | Loss_u: 0.0090 | W: 10.5102\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.009s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0691 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7603 | top1:  79.5000 | top5:  97.3200\n",
      "Test Stats  |################################| (157/157) Data: 0.000s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8031 | top1:  78.5700 | top5:  96.8500\n",
      "\n",
      "Epoch: [145 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.017s | Batch: 0.063s | Total: 0:01:04 | ETA: 0:00:01 | Loss: 0.7236 | Loss_x: 0.6295 | Loss_u: 0.0089 | W: 10.5835\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0713 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7586 | top1:  80.0400 | top5:  96.8800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8042 | top1:  78.9000 | top5:  96.5900\n",
      "\n",
      "Epoch: [146 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.064s | Total: 0:01:05 | ETA: 0:00:01 | Loss: 0.7291 | Loss_x: 0.6329 | Loss_u: 0.0090 | W: 10.6567\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0685 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7670 | top1:  79.5800 | top5:  97.0800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8117 | top1:  78.8300 | top5:  96.7100\n",
      "\n",
      "Epoch: [147 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:08 | ETA: 0:00:01 | Loss: 0.7299 | Loss_x: 0.6347 | Loss_u: 0.0089 | W: 10.7299\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0708 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7719 | top1:  80.0400 | top5:  96.8600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8126 | top1:  78.8200 | top5:  96.5800\n",
      "\n",
      "Epoch: [148 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.069s | Total: 0:01:10 | ETA: 0:00:01 | Loss: 0.7236 | Loss_x: 0.6283 | Loss_u: 0.0088 | W: 10.8032\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0721 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7818 | top1:  79.5200 | top5:  96.8400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8221 | top1:  78.8700 | top5:  96.3900\n",
      "\n",
      "Epoch: [149 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.068s | Total: 0:01:09 | ETA: 0:00:01 | Loss: 0.7286 | Loss_x: 0.6328 | Loss_u: 0.0088 | W: 10.8764\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0724 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7848 | top1:  79.2600 | top5:  96.9200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8230 | top1:  78.4000 | top5:  96.5000\n",
      "\n",
      "Epoch: [150 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.068s | Total: 0:01:09 | ETA: 0:00:01 | Loss: 0.7491 | Loss_x: 0.6482 | Loss_u: 0.0092 | W: 10.9497\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0714 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7742 | top1:  79.6200 | top5:  97.1400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8108 | top1:  78.9500 | top5:  96.6000\n",
      "\n",
      "Epoch: [151 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.068s | Total: 0:01:09 | ETA: 0:00:01 | Loss: 0.7435 | Loss_x: 0.6439 | Loss_u: 0.0090 | W: 11.0229\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.011s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0715 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7788 | top1:  79.1800 | top5:  97.0600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8074 | top1:  78.9100 | top5:  96.4300\n",
      "\n",
      "Epoch: [152 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7359 | Loss_x: 0.6369 | Loss_u: 0.0089 | W: 11.0962\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.011s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0726 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7841 | top1:  79.1400 | top5:  96.8400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8132 | top1:  78.7700 | top5:  96.6300\n",
      "\n",
      "Epoch: [153 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7440 | Loss_x: 0.6441 | Loss_u: 0.0089 | W: 11.1694\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.011s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0759 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7785 | top1:  79.3200 | top5:  96.9000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8105 | top1:  79.0400 | top5:  96.6100\n",
      "\n",
      "Epoch: [154 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7261 | Loss_x: 0.6281 | Loss_u: 0.0087 | W: 11.2426\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0732 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7744 | top1:  79.5400 | top5:  96.8400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.8010 | top1:  79.1700 | top5:  96.5100\n",
      "\n",
      "Epoch: [155 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.068s | Total: 0:01:09 | ETA: 0:00:01 | Loss: 0.7325 | Loss_x: 0.6330 | Loss_u: 0.0088 | W: 11.3159\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0738 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7687 | top1:  79.7800 | top5:  97.1800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7940 | top1:  79.4400 | top5:  96.6000\n",
      "\n",
      "Epoch: [156 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7292 | Loss_x: 0.6282 | Loss_u: 0.0089 | W: 11.3891\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0709 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7608 | top1:  79.6200 | top5:  97.0200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7899 | top1:  79.4200 | top5:  96.6200\n",
      "\n",
      "Epoch: [157 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7336 | Loss_x: 0.6324 | Loss_u: 0.0088 | W: 11.4624\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0684 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7542 | top1:  80.0600 | top5:  97.2000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7874 | top1:  79.3400 | top5:  96.7000\n",
      "\n",
      "Epoch: [158 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7499 | Loss_x: 0.6469 | Loss_u: 0.0089 | W: 11.5356\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0772 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7430 | top1:  80.1200 | top5:  97.3000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7675 | top1:  79.4400 | top5:  96.9500\n",
      "\n",
      "Epoch: [159 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7346 | Loss_x: 0.6323 | Loss_u: 0.0088 | W: 11.6089\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0769 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7519 | top1:  80.0000 | top5:  96.9800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7804 | top1:  79.2500 | top5:  96.6400\n",
      "\n",
      "Epoch: [160 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7504 | Loss_x: 0.6464 | Loss_u: 0.0089 | W: 11.6821\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0772 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7613 | top1:  79.6400 | top5:  96.9400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7983 | top1:  78.8000 | top5:  96.6300\n",
      "\n",
      "Epoch: [161 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.067s | Total: 0:01:08 | ETA: 0:00:01 | Loss: 0.7491 | Loss_x: 0.6437 | Loss_u: 0.0090 | W: 11.7553\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0729 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7576 | top1:  80.1400 | top5:  97.0400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7909 | top1:  78.9700 | top5:  96.5600\n",
      "\n",
      "Epoch: [162 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7371 | Loss_x: 0.6328 | Loss_u: 0.0088 | W: 11.8286\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0785 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7617 | top1:  80.0000 | top5:  97.0400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7934 | top1:  79.1400 | top5:  96.6300\n",
      "\n",
      "Epoch: [163 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.067s | Total: 0:01:08 | ETA: 0:00:01 | Loss: 0.7393 | Loss_x: 0.6342 | Loss_u: 0.0088 | W: 11.9018\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0738 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7660 | top1:  79.5000 | top5:  96.9600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7994 | top1:  78.8100 | top5:  96.8100\n",
      "\n",
      "Epoch: [164 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7503 | Loss_x: 0.6435 | Loss_u: 0.0089 | W: 11.9751\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0761 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7540 | top1:  80.1400 | top5:  96.9600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7918 | top1:  79.4100 | top5:  96.6300\n",
      "\n",
      "Epoch: [165 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.066s | Total: 0:01:07 | ETA: 0:00:01 | Loss: 0.7378 | Loss_x: 0.6320 | Loss_u: 0.0088 | W: 12.0483\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0757 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7387 | top1:  80.2200 | top5:  96.7600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7746 | top1:  79.7500 | top5:  96.7900\n",
      "\n",
      "Epoch: [166 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.067s | Total: 0:01:08 | ETA: 0:00:01 | Loss: 0.7591 | Loss_x: 0.6497 | Loss_u: 0.0090 | W: 12.1215\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.011s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0785 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7411 | top1:  80.4400 | top5:  96.9000\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7857 | top1:  79.4400 | top5:  96.7000\n",
      "\n",
      "Epoch: [167 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.067s | Total: 0:01:08 | ETA: 0:00:01 | Loss: 0.7378 | Loss_x: 0.6323 | Loss_u: 0.0087 | W: 12.1948\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0759 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7438 | top1:  80.0600 | top5:  97.2600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7841 | top1:  79.3200 | top5:  96.6500\n",
      "\n",
      "Epoch: [168 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.068s | Total: 0:01:09 | ETA: 0:00:01 | Loss: 0.7299 | Loss_x: 0.6235 | Loss_u: 0.0087 | W: 12.2680\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0746 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7497 | top1:  80.2200 | top5:  97.1200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7880 | top1:  79.3800 | top5:  96.5200\n",
      "\n",
      "Epoch: [169 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.018s | Batch: 0.065s | Total: 0:01:06 | ETA: 0:00:01 | Loss: 0.7362 | Loss_x: 0.6285 | Loss_u: 0.0087 | W: 12.3413\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0732 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7397 | top1:  80.6400 | top5:  96.9800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7710 | top1:  80.0300 | top5:  96.5600\n",
      "\n",
      "Epoch: [170 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.068s | Total: 0:01:09 | ETA: 0:00:01 | Loss: 0.7271 | Loss_x: 0.6230 | Loss_u: 0.0084 | W: 12.4145\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0718 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7456 | top1:  80.5200 | top5:  97.0600\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7816 | top1:  79.6900 | top5:  96.5800\n",
      "\n",
      "Epoch: [171 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.070s | Total: 0:01:11 | ETA: 0:00:01 | Loss: 0.7409 | Loss_x: 0.6327 | Loss_u: 0.0087 | W: 12.4878\n",
      "Train Stats |################################| (3/3) Data: 0.006s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0776 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7423 | top1:  80.3400 | top5:  97.1800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7751 | top1:  79.5200 | top5:  96.9800\n",
      "\n",
      "Epoch: [172 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.068s | Total: 0:01:09 | ETA: 0:00:01 | Loss: 0.7651 | Loss_x: 0.6517 | Loss_u: 0.0090 | W: 12.5610\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0772 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7209 | top1:  81.0000 | top5:  97.2400\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7611 | top1:  79.8500 | top5:  96.9800\n",
      "\n",
      "Epoch: [173 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.067s | Total: 0:01:08 | ETA: 0:00:01 | Loss: 0.7447 | Loss_x: 0.6346 | Loss_u: 0.0087 | W: 12.6342\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.011s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0765 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7218 | top1:  81.1400 | top5:  97.1200\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7525 | top1:  80.1300 | top5:  96.9000\n",
      "\n",
      "Epoch: [174 | 1024] LR: 0.002000\n",
      "Training |################################| (1024/1024) Data: 0.019s | Batch: 0.069s | Total: 0:01:10 | ETA: 0:00:01 | Loss: 0.7453 | Loss_x: 0.6355 | Loss_u: 0.0086 | W: 12.7075\n",
      "Train Stats |################################| (3/3) Data: 0.007s | Batch: 0.010s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.0773 | top1:  100.0000 | top5:  100.0000\n",
      "Valid Stats |################################| (79/79) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7280 | top1:  80.9000 | top5:  97.0800\n",
      "Test Stats  |################################| (157/157) Data: 0.001s | Batch: 0.004s | Total: 0:00:00 | ETA: 0:00:01 | Loss: 0.7571 | top1:  80.0300 | top5:  96.9300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python MixMatch-pytorch-master/train.py --gpu 0 --n-labeled 250 --out cifar10@250"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "017407799dfb41df4610c10c505ffbbde9027efa8feac0bb014f996d69132d52"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
